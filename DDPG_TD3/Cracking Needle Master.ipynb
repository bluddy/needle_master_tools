{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sloving Needle Master with Twin Delayed DDPG (TD3)\n",
    "Code modified from https://github.com/nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from environment import Environment\n",
    "import utils\n",
    "import TD3\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    policy_name = \"TD3\"\n",
    "    env_name = \"Needle Master\"\n",
    "    seed = 0\n",
    "    start_timesteps = 2e5  # How many time steps purely random policy is run for\n",
    "    eval_freq = 5e3  # How often (time steps) we evaluate\n",
    "    max_timesteps = 1e7   # Max time steps to run environment for\n",
    "    save_models = \"store\"\n",
    "    expl_noise = 1    # Std of Gaussian exploration noise\n",
    "    batch_size = 100\n",
    "    discount = 0.99   # Discount factor\n",
    "    tau = 0.005         # Target network update rate\n",
    "    policy_noise = 0.2   # Noise added to target policy during critic update\n",
    "    noise_clip = 0.5\n",
    "    policy_freq = 2  # Frequency of delayed policy updates\n",
    "    max_size = 5e3\n",
    "    filename = 'environment_14.txt'\n",
    "    \n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(random.randint(1, 10000))\n",
    "if torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed(random.randint(1, 10000))\n",
    "    torch.backends.cudnn.enabled = False  # Disable nondeterministic ops (not sure if critical but better safe than sorry)\n",
    "else:\n",
    "    args.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/home/lifan/workspace/RL/needle_master_tools/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes= 3):\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.select_action(state)\n",
    "            next_state, reward, done = env.step(action, save_image=False)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "    env.episode_reward = avg_reward\n",
    "    frame = env.render(save_image=True, save_path=save_path)\n",
    "\n",
    "    print (\"---------------------------------------\")\n",
    "    print (\"Episode_num: %d, Evaluation over %d episodes: %f\" % (env.episode_num, eval_episodes, avg_reward))\n",
    "    print (\"---------------------------------------\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_Needle Master_0_1\n",
      "---------------------------------------\n",
      "Total T: 82 Episode Num: 1 Episode T: 82 Reward: 16.787760\n",
      "Total T: 126 Episode Num: 2 Episode T: 44 Reward: 265.263533\n",
      "Total T: 146 Episode Num: 3 Episode T: 20 Reward: 0.885530\n",
      "Total T: 230 Episode Num: 4 Episode T: 84 Reward: 110.035524\n",
      "Total T: 310 Episode Num: 5 Episode T: 80 Reward: -126.826092\n",
      "Total T: 389 Episode Num: 6 Episode T: 79 Reward: 126.739138\n",
      "Total T: 414 Episode Num: 7 Episode T: 25 Reward: -7.360321\n",
      "Total T: 495 Episode Num: 8 Episode T: 81 Reward: -126.175066\n",
      "Total T: 579 Episode Num: 9 Episode T: 84 Reward: 120.108929\n",
      "Total T: 608 Episode Num: 10 Episode T: 29 Reward: -13.255395\n",
      "Total T: 645 Episode Num: 11 Episode T: 37 Reward: -30.155327\n",
      "Total T: 728 Episode Num: 12 Episode T: 83 Reward: 14.810490\n",
      "Total T: 810 Episode Num: 13 Episode T: 82 Reward: 110.509970\n",
      "Total T: 899 Episode Num: 14 Episode T: 89 Reward: 10.899278\n",
      "Total T: 982 Episode Num: 15 Episode T: 83 Reward: -125.923172\n",
      "Total T: 1065 Episode Num: 16 Episode T: 83 Reward: 108.216461\n",
      "Total T: 1153 Episode Num: 17 Episode T: 88 Reward: -138.719273\n",
      "Total T: 1192 Episode Num: 18 Episode T: 39 Reward: 263.894054\n",
      "Total T: 1240 Episode Num: 19 Episode T: 48 Reward: -9.784895\n",
      "Total T: 1315 Episode Num: 20 Episode T: 75 Reward: 123.266926\n",
      "Total T: 1378 Episode Num: 21 Episode T: 63 Reward: 155.038437\n",
      "Total T: 1465 Episode Num: 22 Episode T: 87 Reward: 101.362325\n",
      "Total T: 1553 Episode Num: 23 Episode T: 88 Reward: 86.428011\n",
      "Total T: 1577 Episode Num: 24 Episode T: 24 Reward: -5.334992\n",
      "Total T: 1659 Episode Num: 25 Episode T: 82 Reward: 15.384719\n",
      "Total T: 1676 Episode Num: 26 Episode T: 17 Reward: 4.021878\n",
      "Total T: 1758 Episode Num: 27 Episode T: 82 Reward: -127.488985\n",
      "Total T: 1782 Episode Num: 28 Episode T: 24 Reward: -4.336322\n",
      "Total T: 1803 Episode Num: 29 Episode T: 21 Reward: 1.010832\n",
      "Total T: 1854 Episode Num: 30 Episode T: 51 Reward: 74.311711\n",
      "Total T: 1947 Episode Num: 31 Episode T: 93 Reward: -140.506556\n",
      "Total T: 1999 Episode Num: 32 Episode T: 52 Reward: -63.912470\n",
      "Total T: 2088 Episode Num: 33 Episode T: 89 Reward: 92.437902\n",
      "Total T: 2169 Episode Num: 34 Episode T: 81 Reward: -127.178184\n",
      "Total T: 2258 Episode Num: 35 Episode T: 89 Reward: 103.358684\n",
      "Total T: 2339 Episode Num: 36 Episode T: 81 Reward: -127.889667\n",
      "Total T: 2424 Episode Num: 37 Episode T: 85 Reward: -130.430315\n",
      "Total T: 2445 Episode Num: 38 Episode T: 21 Reward: -0.035021\n",
      "Total T: 2527 Episode Num: 39 Episode T: 82 Reward: 115.494447\n",
      "Total T: 2571 Episode Num: 40 Episode T: 44 Reward: -43.132177\n",
      "Total T: 2650 Episode Num: 41 Episode T: 79 Reward: -128.325297\n",
      "Total T: 2732 Episode Num: 42 Episode T: 82 Reward: -126.888205\n",
      "Total T: 2814 Episode Num: 43 Episode T: 82 Reward: -128.994685\n",
      "Total T: 2833 Episode Num: 44 Episode T: 19 Reward: 2.164336\n",
      "Total T: 2863 Episode Num: 45 Episode T: 30 Reward: -18.635134\n",
      "Total T: 2892 Episode Num: 46 Episode T: 29 Reward: -16.757373\n",
      "Total T: 2908 Episode Num: 47 Episode T: 16 Reward: 4.332699\n",
      "Total T: 2992 Episode Num: 48 Episode T: 84 Reward: -127.783871\n",
      "Total T: 3043 Episode Num: 49 Episode T: 51 Reward: 175.657745\n",
      "Total T: 3059 Episode Num: 50 Episode T: 16 Reward: 5.284484\n",
      "Total T: 3142 Episode Num: 51 Episode T: 83 Reward: -128.239728\n",
      "Total T: 3182 Episode Num: 52 Episode T: 40 Reward: 91.740056\n",
      "Total T: 3229 Episode Num: 53 Episode T: 47 Reward: -53.219119\n",
      "Total T: 3310 Episode Num: 54 Episode T: 81 Reward: 115.871128\n",
      "Total T: 3367 Episode Num: 55 Episode T: 57 Reward: 165.886358\n",
      "Total T: 3450 Episode Num: 56 Episode T: 83 Reward: 17.094610\n",
      "Total T: 3533 Episode Num: 57 Episode T: 83 Reward: 17.475200\n",
      "Total T: 3616 Episode Num: 58 Episode T: 83 Reward: 100.685345\n",
      "Total T: 3633 Episode Num: 59 Episode T: 17 Reward: 5.402667\n",
      "Total T: 3674 Episode Num: 60 Episode T: 41 Reward: -38.375988\n",
      "Total T: 3694 Episode Num: 61 Episode T: 20 Reward: 2.827986\n",
      "Total T: 3776 Episode Num: 62 Episode T: 82 Reward: 119.341499\n",
      "Total T: 3829 Episode Num: 63 Episode T: 53 Reward: 170.029108\n",
      "Total T: 3883 Episode Num: 64 Episode T: 54 Reward: 66.364995\n",
      "Total T: 3966 Episode Num: 65 Episode T: 83 Reward: 119.546500\n",
      "Total T: 4051 Episode Num: 66 Episode T: 85 Reward: 118.072351\n",
      "Total T: 4140 Episode Num: 67 Episode T: 89 Reward: 94.528466\n",
      "Total T: 4235 Episode Num: 68 Episode T: 95 Reward: 1.014409\n",
      "Total T: 4327 Episode Num: 69 Episode T: 92 Reward: 107.913875\n",
      "Total T: 4421 Episode Num: 70 Episode T: 94 Reward: 95.165286\n",
      "Total T: 4437 Episode Num: 71 Episode T: 16 Reward: 5.275372\n",
      "Total T: 4478 Episode Num: 72 Episode T: 41 Reward: 193.226819\n",
      "Total T: 4503 Episode Num: 73 Episode T: 25 Reward: -6.885212\n",
      "Total T: 4591 Episode Num: 74 Episode T: 88 Reward: -132.143905\n",
      "Total T: 4674 Episode Num: 75 Episode T: 83 Reward: 98.357795\n",
      "Total T: 4755 Episode Num: 76 Episode T: 81 Reward: 14.361302\n",
      "Total T: 4792 Episode Num: 77 Episode T: 37 Reward: 196.692886\n",
      "Total T: 4864 Episode Num: 78 Episode T: 72 Reward: 33.322197\n",
      "Total T: 4886 Episode Num: 79 Episode T: 22 Reward: -2.212097\n",
      "Total T: 4975 Episode Num: 80 Episode T: 89 Reward: 71.515958\n",
      "---------------------------------------\n",
      "Episode_num: 81, Evaluation over 3 episodes: -128.288632\n",
      "---------------------------------------\n",
      "Total T: 5090 Episode Num: 82 Episode T: 87 Reward: 111.818012\n",
      "Total T: 5189 Episode Num: 83 Episode T: 99 Reward: -3.062452\n",
      "Total T: 5273 Episode Num: 84 Episode T: 84 Reward: 112.809830\n",
      "Total T: 5357 Episode Num: 85 Episode T: 84 Reward: 120.305861\n",
      "Total T: 5445 Episode Num: 86 Episode T: 88 Reward: -9.644267\n",
      "Total T: 5496 Episode Num: 87 Episode T: 51 Reward: -58.099349\n",
      "Total T: 5579 Episode Num: 88 Episode T: 83 Reward: 115.629124\n",
      "Total T: 5599 Episode Num: 89 Episode T: 20 Reward: 0.943269\n",
      "Total T: 5689 Episode Num: 90 Episode T: 90 Reward: 97.694034\n",
      "Total T: 5710 Episode Num: 91 Episode T: 21 Reward: 0.211380\n",
      "Total T: 5743 Episode Num: 92 Episode T: 33 Reward: -22.993243\n",
      "Total T: 5828 Episode Num: 93 Episode T: 85 Reward: 115.208179\n",
      "Total T: 5910 Episode Num: 94 Episode T: 82 Reward: 105.375707\n",
      "Total T: 5996 Episode Num: 95 Episode T: 86 Reward: -151.112481\n",
      "Total T: 6047 Episode Num: 96 Episode T: 51 Reward: -15.561938\n",
      "Total T: 6097 Episode Num: 97 Episode T: 50 Reward: -17.999311\n",
      "Total T: 6177 Episode Num: 98 Episode T: 80 Reward: -127.498498\n",
      "Total T: 6261 Episode Num: 99 Episode T: 84 Reward: 101.270316\n",
      "Total T: 6296 Episode Num: 100 Episode T: 35 Reward: 204.619551\n",
      "Total T: 6382 Episode Num: 101 Episode T: 86 Reward: 97.630748\n",
      "Total T: 6400 Episode Num: 102 Episode T: 18 Reward: 3.745171\n",
      "Total T: 6454 Episode Num: 103 Episode T: 54 Reward: -35.607556\n",
      "Total T: 6508 Episode Num: 104 Episode T: 54 Reward: 170.111749\n",
      "Total T: 6593 Episode Num: 105 Episode T: 85 Reward: 98.796920\n",
      "Total T: 6612 Episode Num: 106 Episode T: 19 Reward: 3.361328\n",
      "Total T: 6656 Episode Num: 107 Episode T: 44 Reward: 191.734913\n",
      "Total T: 6740 Episode Num: 108 Episode T: 84 Reward: 120.975923\n",
      "Total T: 6770 Episode Num: 109 Episode T: 30 Reward: -15.440635\n",
      "Total T: 6856 Episode Num: 110 Episode T: 86 Reward: -117.198014\n",
      "Total T: 6948 Episode Num: 111 Episode T: 92 Reward: 79.117057\n",
      "Total T: 6978 Episode Num: 112 Episode T: 30 Reward: -17.336408\n",
      "Total T: 7011 Episode Num: 113 Episode T: 33 Reward: 104.542846\n",
      "Total T: 7092 Episode Num: 114 Episode T: 81 Reward: 15.576966\n",
      "Total T: 7174 Episode Num: 115 Episode T: 82 Reward: 119.861176\n",
      "Total T: 7261 Episode Num: 116 Episode T: 87 Reward: 104.847264\n",
      "Total T: 7306 Episode Num: 117 Episode T: 45 Reward: -5.578692\n",
      "Total T: 7390 Episode Num: 118 Episode T: 84 Reward: 111.989686\n",
      "Total T: 7430 Episode Num: 119 Episode T: 40 Reward: -35.260590\n",
      "Total T: 7452 Episode Num: 120 Episode T: 22 Reward: -1.049555\n",
      "Total T: 7531 Episode Num: 121 Episode T: 79 Reward: 14.527894\n",
      "Total T: 7584 Episode Num: 122 Episode T: 53 Reward: -26.382635\n",
      "Total T: 7665 Episode Num: 123 Episode T: 81 Reward: 14.021181\n",
      "Total T: 7684 Episode Num: 124 Episode T: 19 Reward: 2.377336\n",
      "Total T: 7765 Episode Num: 125 Episode T: 81 Reward: 119.873200\n",
      "Total T: 7805 Episode Num: 126 Episode T: 40 Reward: 195.461271\n",
      "Total T: 7889 Episode Num: 127 Episode T: 84 Reward: 110.393240\n",
      "Total T: 7973 Episode Num: 128 Episode T: 84 Reward: 120.315432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 8058 Episode Num: 129 Episode T: 85 Reward: 118.965170\n",
      "Total T: 8145 Episode Num: 130 Episode T: 87 Reward: 16.655809\n",
      "Total T: 8175 Episode Num: 131 Episode T: 30 Reward: 206.044144\n",
      "Total T: 8256 Episode Num: 132 Episode T: 81 Reward: 115.138813\n",
      "Total T: 8337 Episode Num: 133 Episode T: 81 Reward: -127.336912\n",
      "Total T: 8354 Episode Num: 134 Episode T: 17 Reward: 4.636181\n",
      "Total T: 8440 Episode Num: 135 Episode T: 86 Reward: 99.243077\n",
      "Total T: 8475 Episode Num: 136 Episode T: 35 Reward: -27.737042\n",
      "Total T: 8497 Episode Num: 137 Episode T: 22 Reward: -0.926884\n",
      "Total T: 8579 Episode Num: 138 Episode T: 82 Reward: 116.226994\n",
      "Total T: 8601 Episode Num: 139 Episode T: 22 Reward: -1.867284\n",
      "Total T: 8653 Episode Num: 140 Episode T: 52 Reward: -40.771620\n",
      "Total T: 8716 Episode Num: 141 Episode T: 63 Reward: -85.265404\n",
      "Total T: 8754 Episode Num: 142 Episode T: 38 Reward: 197.708735\n",
      "Total T: 8837 Episode Num: 143 Episode T: 83 Reward: -127.507280\n",
      "Total T: 8872 Episode Num: 144 Episode T: 35 Reward: -25.247726\n",
      "Total T: 8908 Episode Num: 145 Episode T: 36 Reward: -28.481959\n",
      "Total T: 8957 Episode Num: 146 Episode T: 49 Reward: -106.492649\n",
      "Total T: 8993 Episode Num: 147 Episode T: 36 Reward: 99.429751\n",
      "Total T: 9083 Episode Num: 148 Episode T: 90 Reward: 91.694953\n",
      "Total T: 9176 Episode Num: 149 Episode T: 93 Reward: -136.523617\n",
      "Total T: 9254 Episode Num: 150 Episode T: 78 Reward: 93.115516\n",
      "Total T: 9286 Episode Num: 151 Episode T: 32 Reward: -19.702622\n",
      "Total T: 9370 Episode Num: 152 Episode T: 84 Reward: 118.699189\n",
      "Total T: 9393 Episode Num: 153 Episode T: 23 Reward: -1.101487\n",
      "Total T: 9480 Episode Num: 154 Episode T: 87 Reward: -129.072891\n",
      "Total T: 9563 Episode Num: 155 Episode T: 83 Reward: -128.049103\n",
      "Total T: 9645 Episode Num: 156 Episode T: 82 Reward: -128.226733\n",
      "Total T: 9659 Episode Num: 157 Episode T: 14 Reward: 6.537077\n",
      "Total T: 9744 Episode Num: 158 Episode T: 85 Reward: 117.060793\n",
      "Total T: 9765 Episode Num: 159 Episode T: 21 Reward: 0.242935\n",
      "Total T: 9853 Episode Num: 160 Episode T: 88 Reward: 113.802686\n",
      "Total T: 9938 Episode Num: 161 Episode T: 85 Reward: 120.201243\n",
      "---------------------------------------\n",
      "Episode_num: 162, Evaluation over 3 episodes: -60.439138\n",
      "---------------------------------------\n",
      "Total T: 10055 Episode Num: 163 Episode T: 32 Reward: -19.035455\n",
      "Total T: 10078 Episode Num: 164 Episode T: 23 Reward: -4.025779\n",
      "Total T: 10117 Episode Num: 165 Episode T: 39 Reward: 94.344232\n",
      "Total T: 10199 Episode Num: 166 Episode T: 82 Reward: -127.738439\n",
      "Total T: 10258 Episode Num: 167 Episode T: 59 Reward: 168.181407\n",
      "Total T: 10343 Episode Num: 168 Episode T: 85 Reward: 113.430376\n",
      "Total T: 10436 Episode Num: 169 Episode T: 93 Reward: -144.622394\n",
      "Total T: 10521 Episode Num: 170 Episode T: 85 Reward: 16.931371\n",
      "Total T: 10570 Episode Num: 171 Episode T: 49 Reward: -120.914279\n",
      "Total T: 10652 Episode Num: 172 Episode T: 82 Reward: -126.990722\n",
      "Total T: 10699 Episode Num: 173 Episode T: 47 Reward: 264.853178\n",
      "Total T: 10752 Episode Num: 174 Episode T: 53 Reward: 168.366203\n",
      "Total T: 10822 Episode Num: 175 Episode T: 70 Reward: 134.875070\n",
      "Total T: 10847 Episode Num: 176 Episode T: 25 Reward: -6.118051\n",
      "Total T: 10871 Episode Num: 177 Episode T: 24 Reward: -5.347741\n",
      "Total T: 10965 Episode Num: 178 Episode T: 94 Reward: 110.674739\n",
      "Total T: 11051 Episode Num: 179 Episode T: 86 Reward: 12.863918\n",
      "Total T: 11137 Episode Num: 180 Episode T: 86 Reward: 119.952561\n",
      "Total T: 11159 Episode Num: 181 Episode T: 22 Reward: -2.693397\n",
      "Total T: 11194 Episode Num: 182 Episode T: 35 Reward: 200.323184\n",
      "Total T: 11216 Episode Num: 183 Episode T: 22 Reward: -1.572605\n",
      "Total T: 11239 Episode Num: 184 Episode T: 23 Reward: -3.468386\n",
      "Total T: 11320 Episode Num: 185 Episode T: 81 Reward: 115.874639\n",
      "Total T: 11345 Episode Num: 186 Episode T: 25 Reward: -7.047787\n",
      "Total T: 11362 Episode Num: 187 Episode T: 17 Reward: 4.190273\n",
      "Total T: 11375 Episode Num: 188 Episode T: 13 Reward: 7.074244\n",
      "Total T: 11465 Episode Num: 189 Episode T: 90 Reward: 98.151530\n",
      "Total T: 11558 Episode Num: 190 Episode T: 93 Reward: -154.025811\n",
      "Total T: 11580 Episode Num: 191 Episode T: 22 Reward: -1.700270\n",
      "Total T: 11594 Episode Num: 192 Episode T: 14 Reward: 5.651244\n",
      "Total T: 11675 Episode Num: 193 Episode T: 81 Reward: -127.265133\n",
      "Total T: 11712 Episode Num: 194 Episode T: 37 Reward: -29.372739\n",
      "Total T: 11729 Episode Num: 195 Episode T: 17 Reward: 4.619470\n",
      "Total T: 11816 Episode Num: 196 Episode T: 87 Reward: 104.538757\n",
      "Total T: 11841 Episode Num: 197 Episode T: 25 Reward: -9.336884\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s_%s_%s\" % (args.policy_name, args.env_name, str(args.seed),1)\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "if not os.path.exists(\"./results\"):\n",
    "    os.makedirs(\"./results\")\n",
    "if not os.path.exists(\"./pytorch_models\"):\n",
    "    os.makedirs(\"./pytorch_models\")\n",
    "\n",
    "## environment set up\n",
    "env = Environment(filename=args.filename)\n",
    "env.GetTargetPoint()\n",
    "state_dim = 10\n",
    "action_dim = 2\n",
    "\n",
    "\"\"\"\"  for PID controller \"\"\"\n",
    "action_constrain = [10, np.pi/20]\n",
    "parameter = [0.1,0.0009]\n",
    "\n",
    "\"\"\" [lower bound],[higher bound] \"\"\"\n",
    "env.action_bound = np.array(([-1, -1],[1, 1]))  ## modified lower bound\n",
    "max_action = 1.0\n",
    "\n",
    "### for plotting\n",
    "Reward = []\n",
    "save_path = './out/'\n",
    "\n",
    "# Initialize policy\n",
    "policy = TD3.TD3(state_dim, action_dim, max_action)\n",
    "replay_buffer = utils.ReplayBuffer(args.max_size)\n",
    "\n",
    "# Evaluate untrained policy\n",
    "# evaluations = [evaluate_policy(policy)]\n",
    "\"\"\" start straightly \"\"\"\n",
    "evaluations = []\n",
    "\n",
    "env.total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "done = True\n",
    "\n",
    "while env.total_timesteps < args.max_timesteps:\n",
    "\n",
    "    ## finish one episode, and train episode_times\n",
    "    if done:\n",
    "\n",
    "        # Evaluate episode\n",
    "        if timesteps_since_eval >= args.eval_freq:\n",
    "            timesteps_since_eval %= args.eval_freq\n",
    "            evaluations.append(evaluate_policy(policy))\n",
    "\n",
    "            policy.save(file_name, directory=\"./pytorch_models\")\n",
    "            np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "\n",
    "        ## load model\n",
    "        # policy.load(file_name,\"./pytorch_models\")\n",
    "\n",
    "        ## training as usual\n",
    "        else:\n",
    "            # if env.total_timesteps != 0 and env.episode_reward > 500:\n",
    "            if env.total_timesteps != 0:\n",
    "                print ((\"Total T: %d Episode Num: %d Episode T: %d Reward: %f\") % (\n",
    "                    env.total_timesteps, env.episode_num, episode_timesteps, env.episode_reward))\n",
    "                frame = env.render(env.episode_num, save_image=True, save_path=save_path)\n",
    "\n",
    "            if env.total_timesteps != 0:\n",
    "                policy.train(replay_buffer, episode_timesteps, args.batch_size, args.discount, args.tau, args.policy_noise, args.noise_clip, args.policy_freq)\n",
    "\n",
    "\n",
    "        Reward.append(env.episode_reward)\n",
    "        plt.plot(np.arange(1, env.episode_num), Reward[1:env.episode_num], 'b')\n",
    "        plt.savefig('./out/episode reward.png')\n",
    "        \n",
    "#         print(\"~~~~~~~~~~~~~~~~~~~~~~~~ round done ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "\n",
    "        done = False\n",
    "\n",
    "        env.episode_num += 1\n",
    "        env.episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        \n",
    "        \"\"\" exploration rate decay \"\"\"\n",
    "        if env.total_timesteps % 1000 == 0 and args.expl_noise > 0:\n",
    "            args.expl_noise -= 0.001 \n",
    "\n",
    "    # Select action randomly or according to policy\n",
    "    if env.total_timesteps % args.max_size < args.start_timesteps:\n",
    "        action = env.sample_action()\n",
    "#         print(\"randomly selected: \" + str(action))\n",
    "        # action = env.PIDcontroller(action_constrain, parameter, env.t)\n",
    "        # print(\"PID controller: \" +str(action))\n",
    "    else:\n",
    "        action = policy.select_action(state)\n",
    "        # print(\"action based on polilcy:\" + str(action))\n",
    "        # print(\"action selected: \" +str(action))\n",
    "        if args.expl_noise != 0:\n",
    "            action = (action + np.random.normal(0, args.expl_noise, size=2)).clip(\n",
    "                env.action_bound[0,:], env.action_bound[1,:])\n",
    "        # print(\"noised action: \" +str(action))\n",
    "\n",
    "\n",
    "\n",
    "    # Perform action\n",
    "    new_state, reward, done = env.step(action, save_image=True)\n",
    "\n",
    "    running = env.check_status()\n",
    "\n",
    "    done_bool = 0 if episode_timesteps + 1 == env.max_time else float(done)\n",
    "    env.episode_reward += reward\n",
    "\n",
    "    # Store data in replay buffer\n",
    "    replay_buffer.add((state, new_state, action, reward, done_bool))\n",
    "#     print(\"state: \" + str(state))\n",
    "    \n",
    "    state = new_state\n",
    "\n",
    "\n",
    "    episode_timesteps += 1\n",
    "    env.total_timesteps += 1\n",
    "    timesteps_since_eval += 1\n",
    "\n",
    "    plt.plot(range(len(policy.actor_loss)), policy.actor_loss)\n",
    "    plt.savefig('./results/actor loss.png')\n",
    "\n",
    "    plt.plot(range(len(policy.critic_loss)), policy.critic_loss)\n",
    "    plt.savefig('./results/critic loss.png')\n",
    "\n",
    "# Final evaluation\n",
    "# plt.plot(np.arange(1,episode_num),Reward[0:episode_num],'b')\n",
    "# plt.savefig('./out/episode reward.png')\n",
    "\n",
    "    # if  env.total_timesteps % 1000 == 0:\n",
    "    #     evaluations.append(evaluate_policy(policy))\n",
    "    #     policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
    "    #     np.save(\"./results/%s\" % (file_name), evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
