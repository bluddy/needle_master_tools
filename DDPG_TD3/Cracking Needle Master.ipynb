{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sloving Needle Master with Twin Delayed DDPG (TD3)\n",
    "Code modified from https://github.com/nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from environment import Environment\n",
    "from environment import PID\n",
    "import utils\n",
    "import TD3_priorized\n",
    "import TD3\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    policy_name = \"TD3\"\n",
    "    env_name = \"Needle Master\"\n",
    "    seed = 1e6\n",
    "    eval_freq = 5e3 # How often (time steps) we evaluate\n",
    "    max_timesteps = 1e6  # Max time steps to run environment for\n",
    "    save_models = \"store\"\n",
    "    expl_noise = 1    # Std of Gaussian exploration noise\n",
    "    batch_size = 100\n",
    "    discount = 0.99   # Discount factor\n",
    "    tau = 0.005         # Target network update rate\n",
    "    policy_noise = 0.2   # Noise added to target policy during critic update\n",
    "    noise_clip = 0.5\n",
    "    policy_freq = 2  # Frequency of delayed policy updates\n",
    "    max_size = 1e6\n",
    "    pid_freq = 9e2    # How often purely random policy is run for\n",
    "    pid_interval = 5e2   # How many time steps purely random policy is run for\n",
    "    filename = 'environment_15'\n",
    "    \n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(random.randint(1, 10000))\n",
    "if torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed(random.randint(1, 10000))\n",
    "    torch.backends.cudnn.enabled = False  # Disable nondeterministic ops (not sure if critical but better safe than sorry)\n",
    "else:\n",
    "    args.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/home/lifan/workspace/RL/needle_master_tools/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, log_f):\n",
    "    eval_path = './evaluate/'\n",
    "    evaluation_time = 3\n",
    "    if not os.path.exists(eval_path):\n",
    "        os.mkdir(eval_path)\n",
    "\n",
    "    state = env.reset(log_f)\n",
    "    done = False\n",
    "    env.episode_num += 1\n",
    "    env.episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    average_reward = 0\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        action = policy.select_action(state)\n",
    "        # print(\"state: \" + str(state))\n",
    "        # print(\"action: \" + str(action))\n",
    "        new_state, reward, done = env.step(action, log_f)\n",
    "        # print(\"next state: \" + str(next_state))\n",
    "        # print(\"done: \" +str(done))\n",
    "        env.episode_reward += reward\n",
    "        state = new_state\n",
    "        episode_timesteps += 1\n",
    "        env.total_timesteps += 1\n",
    "\n",
    "    env.render(save_image=True, save_path=eval_path)\n",
    "\n",
    "    print (\"---------------------------------------\")\n",
    "    print (\"Episode_num: %d: %f\" % (env.episode_num, env.episode_reward))\n",
    "    print (\"---------------------------------------\")\n",
    "    return env.episode_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: environment_15_TD3\n",
      "---------------------------------------\n",
      "Total T: 174 Episode Num: 20 Episode T: 5 Reward: -0.714488\n",
      "Total T: 283 Episode Num: 40 Episode T: 4 Reward: 0.112196\n",
      "Total T: 377 Episode Num: 60 Episode T: 4 Reward: -0.768431\n",
      "Total T: 466 Episode Num: 80 Episode T: 6 Reward: 0.126771\n",
      "Total T: 563 Episode Num: 100 Episode T: 7 Reward: -0.303387\n",
      "Total T: 647 Episode Num: 120 Episode T: 4 Reward: -0.649450\n",
      "Total T: 736 Episode Num: 140 Episode T: 5 Reward: -0.218081\n",
      "Total T: 833 Episode Num: 160 Episode T: 8 Reward: 0.108082\n",
      "Total T: 942 Episode Num: 180 Episode T: 4 Reward: 0.205640\n",
      "Total T: 1041 Episode Num: 200 Episode T: 5 Reward: 0.237422\n",
      "Total T: 1131 Episode Num: 220 Episode T: 5 Reward: 0.372746\n",
      "Total T: 1228 Episode Num: 240 Episode T: 5 Reward: -0.220079\n",
      "Total T: 1336 Episode Num: 260 Episode T: 4 Reward: 0.177994\n",
      "Total T: 1437 Episode Num: 280 Episode T: 3 Reward: -0.430322\n",
      "Total T: 1535 Episode Num: 300 Episode T: 4 Reward: -0.633470\n",
      "Total T: 1628 Episode Num: 320 Episode T: 5 Reward: -0.731350\n",
      "Total T: 1736 Episode Num: 340 Episode T: 3 Reward: -0.814378\n",
      "Total T: 1829 Episode Num: 360 Episode T: 4 Reward: 0.474460\n",
      "Total T: 1941 Episode Num: 380 Episode T: 5 Reward: 0.325773\n",
      "Total T: 2036 Episode Num: 400 Episode T: 5 Reward: -1.088239\n",
      "Total T: 2143 Episode Num: 420 Episode T: 4 Reward: 0.305928\n",
      "Total T: 2260 Episode Num: 440 Episode T: 15 Reward: 0.303940\n",
      "Total T: 2344 Episode Num: 460 Episode T: 4 Reward: -0.848932\n",
      "Total T: 2437 Episode Num: 480 Episode T: 5 Reward: 0.410426\n",
      "Total T: 2536 Episode Num: 500 Episode T: 5 Reward: -0.535543\n",
      "Total T: 2624 Episode Num: 520 Episode T: 4 Reward: -0.895741\n",
      "Total T: 2716 Episode Num: 540 Episode T: 4 Reward: -0.839141\n",
      "Total T: 2804 Episode Num: 560 Episode T: 4 Reward: 0.408180\n",
      "Total T: 2899 Episode Num: 580 Episode T: 5 Reward: -0.399711\n",
      "Total T: 2997 Episode Num: 600 Episode T: 4 Reward: 0.321283\n",
      "Total T: 3111 Episode Num: 620 Episode T: 5 Reward: 0.268443\n",
      "Total T: 3204 Episode Num: 640 Episode T: 5 Reward: 0.301574\n",
      "Total T: 3317 Episode Num: 660 Episode T: 4 Reward: 0.066143\n",
      "Total T: 3402 Episode Num: 680 Episode T: 4 Reward: 0.388628\n",
      "Total T: 3495 Episode Num: 700 Episode T: 4 Reward: -0.709258\n",
      "Total T: 3591 Episode Num: 720 Episode T: 3 Reward: -0.519083\n",
      "Total T: 3688 Episode Num: 740 Episode T: 4 Reward: -0.501488\n",
      "Total T: 3781 Episode Num: 760 Episode T: 3 Reward: -0.356826\n",
      "Total T: 3981 Episode Num: 780 Episode T: 14 Reward: 441.922975\n",
      "Total T: 4253 Episode Num: 800 Episode T: 13 Reward: 441.366666\n",
      "Total T: 4550 Episode Num: 820 Episode T: 12 Reward: 441.604620\n",
      "Total T: 4830 Episode Num: 840 Episode T: 15 Reward: 441.385022\n",
      "---------------------------------------\n",
      "Episode_num: 854: -0.033092\n",
      "---------------------------------------\n",
      "Total T: 5095 Episode Num: 860 Episode T: 14 Reward: 441.770998\n",
      "Total T: 5374 Episode Num: 880 Episode T: 13 Reward: 441.678482\n",
      "Total T: 5668 Episode Num: 900 Episode T: 15 Reward: 440.688243\n",
      "Total T: 5951 Episode Num: 920 Episode T: 12 Reward: 41.717160\n",
      "Total T: 6228 Episode Num: 940 Episode T: 13 Reward: 441.343657\n",
      "Total T: 6500 Episode Num: 960 Episode T: 14 Reward: 440.600590\n",
      "Total T: 6769 Episode Num: 980 Episode T: 17 Reward: 441.309751\n",
      "Total T: 7043 Episode Num: 1000 Episode T: 12 Reward: 441.097070\n",
      "Total T: 7323 Episode Num: 1020 Episode T: 14 Reward: 441.697775\n",
      "Total T: 7606 Episode Num: 1040 Episode T: 14 Reward: 1.815786\n",
      "Total T: 7890 Episode Num: 1060 Episode T: 16 Reward: 441.160193\n",
      "Total T: 8175 Episode Num: 1080 Episode T: 13 Reward: 1.215548\n",
      "Total T: 8443 Episode Num: 1100 Episode T: 12 Reward: 440.381110\n",
      "Total T: 8713 Episode Num: 1120 Episode T: 15 Reward: 441.569976\n",
      "Total T: 9003 Episode Num: 1140 Episode T: 14 Reward: 440.461715\n",
      "Total T: 9276 Episode Num: 1160 Episode T: 17 Reward: 441.800517\n",
      "Total T: 9565 Episode Num: 1180 Episode T: 15 Reward: 441.498783\n",
      "Total T: 9840 Episode Num: 1200 Episode T: 12 Reward: 440.509169\n",
      "---------------------------------------\n",
      "Episode_num: 1214: -0.333601\n",
      "---------------------------------------\n",
      "Total T: 10119 Episode Num: 1220 Episode T: 14 Reward: 441.321414\n",
      "Total T: 10386 Episode Num: 1240 Episode T: 12 Reward: 440.645263\n",
      "Total T: 10657 Episode Num: 1260 Episode T: 13 Reward: 1.190952\n",
      "Total T: 10927 Episode Num: 1280 Episode T: 12 Reward: 441.371047\n",
      "Total T: 11194 Episode Num: 1300 Episode T: 13 Reward: 441.042541\n",
      "Total T: 11457 Episode Num: 1320 Episode T: 13 Reward: 441.177864\n",
      "Total T: 11714 Episode Num: 1340 Episode T: 17 Reward: 981.470075\n",
      "Total T: 11991 Episode Num: 1360 Episode T: 15 Reward: 440.964403\n",
      "Total T: 12262 Episode Num: 1380 Episode T: 15 Reward: 441.651133\n",
      "Total T: 12546 Episode Num: 1400 Episode T: 12 Reward: 1.639521\n",
      "Total T: 12807 Episode Num: 1420 Episode T: 13 Reward: 441.151451\n",
      "Total T: 13081 Episode Num: 1440 Episode T: 12 Reward: 40.645651\n",
      "Total T: 13364 Episode Num: 1460 Episode T: 15 Reward: 441.703043\n",
      "Total T: 13639 Episode Num: 1480 Episode T: 17 Reward: 441.457693\n",
      "Total T: 13913 Episode Num: 1500 Episode T: 14 Reward: 441.435507\n",
      "Total T: 14184 Episode Num: 1520 Episode T: 12 Reward: 441.932484\n",
      "Total T: 14451 Episode Num: 1540 Episode T: 12 Reward: 40.134219\n",
      "Total T: 14716 Episode Num: 1560 Episode T: 13 Reward: 440.361292\n",
      "Total T: 14989 Episode Num: 1580 Episode T: 12 Reward: 1.913428\n",
      "---------------------------------------\n",
      "Episode_num: 1584: -0.831534\n",
      "---------------------------------------\n",
      "Total T: 15266 Episode Num: 1600 Episode T: 16 Reward: 441.430348\n",
      "Total T: 15529 Episode Num: 1620 Episode T: 13 Reward: 0.553993\n",
      "Total T: 15795 Episode Num: 1640 Episode T: 12 Reward: 1.375885\n",
      "Total T: 16066 Episode Num: 1660 Episode T: 11 Reward: 0.474144\n",
      "Total T: 16344 Episode Num: 1680 Episode T: 14 Reward: 440.474667\n",
      "Total T: 16622 Episode Num: 1700 Episode T: 14 Reward: 440.902302\n",
      "Total T: 16886 Episode Num: 1720 Episode T: 13 Reward: 441.912684\n",
      "Total T: 17149 Episode Num: 1740 Episode T: 12 Reward: 1.787339\n",
      "Total T: 17419 Episode Num: 1760 Episode T: 14 Reward: 440.005904\n",
      "Total T: 17698 Episode Num: 1780 Episode T: 18 Reward: 441.217878\n",
      "Total T: 17961 Episode Num: 1800 Episode T: 12 Reward: 40.957042\n",
      "Total T: 18218 Episode Num: 1820 Episode T: 11 Reward: 0.620803\n",
      "Total T: 18475 Episode Num: 1840 Episode T: 11 Reward: 441.153172\n",
      "Total T: 18745 Episode Num: 1860 Episode T: 14 Reward: 439.693137\n",
      "Total T: 19007 Episode Num: 1880 Episode T: 13 Reward: 440.607600\n",
      "Total T: 19273 Episode Num: 1900 Episode T: 15 Reward: 441.172759\n",
      "Total T: 19530 Episode Num: 1920 Episode T: 13 Reward: 441.742291\n",
      "Total T: 19792 Episode Num: 1940 Episode T: 12 Reward: 440.689750\n",
      "---------------------------------------\n",
      "Episode_num: 1961: -0.959975\n",
      "---------------------------------------\n",
      "Total T: 20297 Episode Num: 1980 Episode T: 14 Reward: 441.400045\n",
      "Total T: 20571 Episode Num: 2000 Episode T: 18 Reward: 0.735455\n",
      "Total T: 20834 Episode Num: 2020 Episode T: 12 Reward: 440.555939\n",
      "Total T: 21097 Episode Num: 2040 Episode T: 12 Reward: 1.566461\n",
      "Total T: 21353 Episode Num: 2060 Episode T: 16 Reward: 441.323392\n",
      "Total T: 21611 Episode Num: 2080 Episode T: 12 Reward: 440.357474\n",
      "Total T: 21872 Episode Num: 2100 Episode T: 12 Reward: 0.930364\n",
      "Total T: 22130 Episode Num: 2120 Episode T: 14 Reward: 440.951516\n",
      "Total T: 22382 Episode Num: 2140 Episode T: 12 Reward: -0.103194\n",
      "Total T: 22644 Episode Num: 2160 Episode T: 14 Reward: 441.549070\n",
      "Total T: 22904 Episode Num: 2180 Episode T: 12 Reward: -0.059711\n",
      "Total T: 23166 Episode Num: 2200 Episode T: 11 Reward: 1.271828\n",
      "Total T: 23416 Episode Num: 2220 Episode T: 14 Reward: 439.751533\n",
      "Total T: 23680 Episode Num: 2240 Episode T: 13 Reward: 441.100260\n",
      "Total T: 23926 Episode Num: 2260 Episode T: 13 Reward: 441.513053\n",
      "Total T: 24196 Episode Num: 2280 Episode T: 11 Reward: 0.411141\n",
      "Total T: 24452 Episode Num: 2300 Episode T: 12 Reward: -0.026247\n",
      "Total T: 24700 Episode Num: 2320 Episode T: 13 Reward: 441.434750\n",
      "Total T: 24960 Episode Num: 2340 Episode T: 14 Reward: 439.927261\n",
      "---------------------------------------\n",
      "Episode_num: 2349: -0.980144\n",
      "---------------------------------------\n",
      "Total T: 25215 Episode Num: 2360 Episode T: 14 Reward: 439.438424\n",
      "Total T: 25483 Episode Num: 2380 Episode T: 12 Reward: 1.761182\n",
      "Total T: 25745 Episode Num: 2400 Episode T: 11 Reward: -0.807356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 25997 Episode Num: 2420 Episode T: 13 Reward: 440.297943\n",
      "Total T: 26244 Episode Num: 2440 Episode T: 12 Reward: 1.588981\n",
      "Total T: 26508 Episode Num: 2460 Episode T: 11 Reward: 1.085616\n",
      "Total T: 26759 Episode Num: 2480 Episode T: 12 Reward: 39.294495\n",
      "Total T: 27006 Episode Num: 2500 Episode T: 13 Reward: 1.477102\n",
      "Total T: 27264 Episode Num: 2520 Episode T: 12 Reward: -0.229936\n",
      "Total T: 27528 Episode Num: 2540 Episode T: 14 Reward: 440.066812\n",
      "Total T: 27783 Episode Num: 2560 Episode T: 12 Reward: 0.318469\n",
      "Total T: 28036 Episode Num: 2580 Episode T: 12 Reward: 1.092425\n",
      "Total T: 28296 Episode Num: 2600 Episode T: 12 Reward: -0.490624\n",
      "Total T: 28549 Episode Num: 2620 Episode T: 14 Reward: 441.097484\n",
      "Total T: 28801 Episode Num: 2640 Episode T: 12 Reward: 1.186365\n",
      "Total T: 29056 Episode Num: 2660 Episode T: 13 Reward: 440.379821\n",
      "Total T: 29309 Episode Num: 2680 Episode T: 14 Reward: 439.540512\n",
      "Total T: 29561 Episode Num: 2700 Episode T: 12 Reward: 1.278114\n",
      "Total T: 29811 Episode Num: 2720 Episode T: 12 Reward: 0.777352\n",
      "Total T: 30059 Episode Num: 2740 Episode T: 13 Reward: 439.971089\n",
      "---------------------------------------\n",
      "Episode_num: 2743: -0.976896\n",
      "---------------------------------------\n",
      "Total T: 30308 Episode Num: 2760 Episode T: 11 Reward: 0.115973\n",
      "Total T: 30562 Episode Num: 2780 Episode T: 12 Reward: 440.522869\n",
      "Total T: 30820 Episode Num: 2800 Episode T: 13 Reward: 439.955256\n",
      "Total T: 31067 Episode Num: 2820 Episode T: 13 Reward: 0.198574\n",
      "Total T: 31324 Episode Num: 2840 Episode T: 11 Reward: 0.527117\n",
      "Total T: 31579 Episode Num: 2860 Episode T: 12 Reward: 1.755370\n",
      "Total T: 31839 Episode Num: 2880 Episode T: 13 Reward: -1.111980\n",
      "Total T: 32093 Episode Num: 2900 Episode T: 12 Reward: 0.788159\n",
      "Total T: 32338 Episode Num: 2920 Episode T: 12 Reward: -0.040498\n",
      "Total T: 32590 Episode Num: 2940 Episode T: 5 Reward: 0.478448\n",
      "Total T: 32656 Episode Num: 2960 Episode T: 3 Reward: 0.199036\n",
      "Total T: 32735 Episode Num: 2980 Episode T: 3 Reward: -0.157591\n",
      "Total T: 32810 Episode Num: 3000 Episode T: 3 Reward: -0.680840\n",
      "Total T: 32885 Episode Num: 3020 Episode T: 4 Reward: -1.086146\n",
      "Total T: 32963 Episode Num: 3040 Episode T: 6 Reward: -0.334708\n",
      "Total T: 33032 Episode Num: 3060 Episode T: 8 Reward: -0.979011\n",
      "Total T: 33100 Episode Num: 3080 Episode T: 4 Reward: 0.584275\n",
      "Total T: 33179 Episode Num: 3100 Episode T: 3 Reward: -0.324605\n",
      "Total T: 33246 Episode Num: 3120 Episode T: 4 Reward: -0.985502\n",
      "Total T: 33321 Episode Num: 3140 Episode T: 3 Reward: -0.354226\n",
      "Total T: 33392 Episode Num: 3160 Episode T: 3 Reward: -0.922602\n",
      "Total T: 33459 Episode Num: 3180 Episode T: 3 Reward: -0.217591\n",
      "Total T: 33538 Episode Num: 3200 Episode T: 3 Reward: 0.425812\n",
      "Total T: 33608 Episode Num: 3220 Episode T: 3 Reward: -0.402376\n",
      "Total T: 33676 Episode Num: 3240 Episode T: 3 Reward: 0.553120\n",
      "Total T: 33761 Episode Num: 3260 Episode T: 8 Reward: 439.660807\n",
      "Total T: 33833 Episode Num: 3280 Episode T: 6 Reward: -0.594468\n",
      "Total T: 33904 Episode Num: 3300 Episode T: 3 Reward: 0.051891\n",
      "Total T: 33978 Episode Num: 3320 Episode T: 3 Reward: -0.616350\n",
      "Total T: 34049 Episode Num: 3340 Episode T: 3 Reward: -0.826094\n",
      "Total T: 34116 Episode Num: 3360 Episode T: 3 Reward: -0.874087\n",
      "Total T: 34187 Episode Num: 3380 Episode T: 4 Reward: -0.612123\n",
      "Total T: 34252 Episode Num: 3400 Episode T: 4 Reward: -0.080151\n",
      "Total T: 34332 Episode Num: 3420 Episode T: 3 Reward: -0.783964\n",
      "Total T: 34404 Episode Num: 3440 Episode T: 3 Reward: 0.106715\n",
      "Total T: 34472 Episode Num: 3460 Episode T: 3 Reward: 0.324451\n",
      "Total T: 34540 Episode Num: 3480 Episode T: 3 Reward: -0.379650\n",
      "Total T: 34613 Episode Num: 3500 Episode T: 3 Reward: -0.719286\n",
      "Total T: 34681 Episode Num: 3520 Episode T: 3 Reward: -0.565007\n",
      "Total T: 34762 Episode Num: 3540 Episode T: 3 Reward: -0.783942\n",
      "Total T: 34829 Episode Num: 3560 Episode T: 3 Reward: -0.206755\n",
      "Total T: 34897 Episode Num: 3580 Episode T: 3 Reward: -0.075392\n",
      "Total T: 34967 Episode Num: 3600 Episode T: 3 Reward: -1.026664\n",
      "Total T: 35034 Episode Num: 3620 Episode T: 3 Reward: 0.164951\n",
      "---------------------------------------\n",
      "Episode_num: 3637: -0.326451\n",
      "---------------------------------------\n",
      "Total T: 35101 Episode Num: 3640 Episode T: 3 Reward: -0.361390\n",
      "Total T: 35170 Episode Num: 3660 Episode T: 4 Reward: -0.539754\n",
      "Total T: 35238 Episode Num: 3680 Episode T: 6 Reward: -0.605996\n",
      "Total T: 35312 Episode Num: 3700 Episode T: 3 Reward: -0.659737\n",
      "Total T: 35378 Episode Num: 3720 Episode T: 3 Reward: -0.919076\n",
      "Total T: 35471 Episode Num: 3740 Episode T: 3 Reward: -0.861433\n",
      "Total T: 35547 Episode Num: 3760 Episode T: 3 Reward: -0.334848\n",
      "Total T: 35611 Episode Num: 3780 Episode T: 3 Reward: 0.624065\n",
      "Total T: 35683 Episode Num: 3800 Episode T: 3 Reward: -0.842794\n",
      "Total T: 35752 Episode Num: 3820 Episode T: 3 Reward: -0.574025\n",
      "Total T: 35824 Episode Num: 3840 Episode T: 3 Reward: -0.862039\n",
      "Total T: 35895 Episode Num: 3860 Episode T: 3 Reward: 0.236228\n",
      "Total T: 35973 Episode Num: 3880 Episode T: 5 Reward: 40.106256\n",
      "Total T: 36048 Episode Num: 3900 Episode T: 4 Reward: -0.194567\n",
      "Total T: 36114 Episode Num: 3920 Episode T: 3 Reward: -0.624753\n",
      "Total T: 36187 Episode Num: 3940 Episode T: 4 Reward: 0.188899\n",
      "Total T: 36260 Episode Num: 3960 Episode T: 3 Reward: -0.623783\n",
      "Total T: 36330 Episode Num: 3980 Episode T: 4 Reward: -0.782227\n",
      "Total T: 36403 Episode Num: 4000 Episode T: 3 Reward: -0.263255\n",
      "Total T: 36475 Episode Num: 4020 Episode T: 3 Reward: -0.550672\n",
      "Total T: 36541 Episode Num: 4040 Episode T: 3 Reward: -0.854705\n",
      "Total T: 36607 Episode Num: 4060 Episode T: 3 Reward: -0.960086\n",
      "Total T: 36684 Episode Num: 4080 Episode T: 3 Reward: 0.542313\n",
      "Total T: 36754 Episode Num: 4100 Episode T: 5 Reward: -0.315803\n",
      "Total T: 36825 Episode Num: 4120 Episode T: 5 Reward: -0.654429\n",
      "Total T: 36892 Episode Num: 4140 Episode T: 3 Reward: -0.852822\n",
      "Total T: 36965 Episode Num: 4160 Episode T: 3 Reward: -0.763340\n",
      "Total T: 37031 Episode Num: 4180 Episode T: 4 Reward: -0.600741\n",
      "Total T: 37099 Episode Num: 4200 Episode T: 3 Reward: -0.294018\n",
      "Total T: 37179 Episode Num: 4220 Episode T: 5 Reward: -0.908089\n",
      "Total T: 37251 Episode Num: 4240 Episode T: 4 Reward: 0.166443\n",
      "Total T: 37327 Episode Num: 4260 Episode T: 5 Reward: -0.958149\n",
      "Total T: 37393 Episode Num: 4280 Episode T: 3 Reward: -0.412463\n",
      "Total T: 37464 Episode Num: 4300 Episode T: 3 Reward: -0.442310\n",
      "Total T: 37536 Episode Num: 4320 Episode T: 3 Reward: -1.105336\n",
      "Total T: 37607 Episode Num: 4340 Episode T: 3 Reward: -0.658648\n",
      "Total T: 37675 Episode Num: 4360 Episode T: 3 Reward: -0.236372\n",
      "Total T: 37743 Episode Num: 4380 Episode T: 4 Reward: -1.081292\n",
      "Total T: 37815 Episode Num: 4400 Episode T: 4 Reward: -0.221443\n",
      "Total T: 37890 Episode Num: 4420 Episode T: 9 Reward: 440.455918\n",
      "Total T: 37960 Episode Num: 4440 Episode T: 3 Reward: -0.796481\n",
      "Total T: 38033 Episode Num: 4460 Episode T: 3 Reward: -0.650254\n",
      "Total T: 38106 Episode Num: 4480 Episode T: 3 Reward: -0.748839\n",
      "Total T: 38179 Episode Num: 4500 Episode T: 5 Reward: 0.269486\n",
      "Total T: 38244 Episode Num: 4520 Episode T: 3 Reward: -0.275580\n",
      "Total T: 38310 Episode Num: 4540 Episode T: 3 Reward: 0.406389\n",
      "Total T: 38378 Episode Num: 4560 Episode T: 3 Reward: -0.272928\n",
      "Total T: 38448 Episode Num: 4580 Episode T: 3 Reward: -0.844936\n",
      "Total T: 38512 Episode Num: 4600 Episode T: 3 Reward: 0.050855\n",
      "Total T: 38578 Episode Num: 4620 Episode T: 3 Reward: -0.524776\n",
      "Total T: 38645 Episode Num: 4640 Episode T: 3 Reward: -1.152609\n",
      "Total T: 38711 Episode Num: 4660 Episode T: 3 Reward: -0.821194\n",
      "Total T: 38780 Episode Num: 4680 Episode T: 3 Reward: -0.591667\n",
      "Total T: 38846 Episode Num: 4700 Episode T: 3 Reward: -0.746860\n",
      "Total T: 38917 Episode Num: 4720 Episode T: 4 Reward: -0.933666\n",
      "Total T: 38997 Episode Num: 4740 Episode T: 3 Reward: -0.266591\n",
      "Total T: 39057 Episode Num: 4760 Episode T: 3 Reward: -0.433817\n",
      "Total T: 39127 Episode Num: 4780 Episode T: 3 Reward: 0.342060\n",
      "Total T: 39190 Episode Num: 4800 Episode T: 4 Reward: -0.895871\n",
      "Total T: 39258 Episode Num: 4820 Episode T: 3 Reward: -0.049505\n",
      "Total T: 39320 Episode Num: 4840 Episode T: 3 Reward: 0.365216\n",
      "Total T: 39394 Episode Num: 4860 Episode T: 5 Reward: -0.594410\n",
      "Total T: 39458 Episode Num: 4880 Episode T: 5 Reward: 0.086398\n",
      "Total T: 39527 Episode Num: 4900 Episode T: 3 Reward: -0.488706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 39595 Episode Num: 4920 Episode T: 3 Reward: -0.334006\n",
      "Total T: 39665 Episode Num: 4940 Episode T: 2 Reward: -0.789446\n",
      "Total T: 39731 Episode Num: 4960 Episode T: 4 Reward: 0.036070\n",
      "Total T: 39799 Episode Num: 4980 Episode T: 2 Reward: -0.795202\n",
      "Total T: 39872 Episode Num: 5000 Episode T: 3 Reward: -0.764742\n",
      "Total T: 39943 Episode Num: 5020 Episode T: 7 Reward: 0.145214\n",
      "Total T: 40013 Episode Num: 5040 Episode T: 3 Reward: 0.434810\n",
      "Total T: 40085 Episode Num: 5060 Episode T: 3 Reward: 0.343149\n",
      "---------------------------------------\n",
      "Episode_num: 5063: -0.786185\n",
      "---------------------------------------\n",
      "Total T: 40149 Episode Num: 5080 Episode T: 2 Reward: -0.785759\n",
      "Total T: 40224 Episode Num: 5100 Episode T: 3 Reward: 0.393539\n",
      "Total T: 40290 Episode Num: 5120 Episode T: 3 Reward: 0.639325\n",
      "Total T: 40355 Episode Num: 5140 Episode T: 3 Reward: -0.867455\n",
      "Total T: 40426 Episode Num: 5160 Episode T: 3 Reward: -1.106902\n",
      "Total T: 40489 Episode Num: 5180 Episode T: 3 Reward: -0.118572\n",
      "Total T: 40554 Episode Num: 5200 Episode T: 3 Reward: -0.611835\n",
      "Total T: 40621 Episode Num: 5220 Episode T: 3 Reward: -0.235732\n",
      "Total T: 40688 Episode Num: 5240 Episode T: 3 Reward: -0.557181\n",
      "Total T: 40751 Episode Num: 5260 Episode T: 2 Reward: -0.781164\n",
      "Total T: 40822 Episode Num: 5280 Episode T: 3 Reward: 0.160437\n",
      "Total T: 40888 Episode Num: 5300 Episode T: 3 Reward: -0.231732\n",
      "Total T: 40954 Episode Num: 5320 Episode T: 5 Reward: -0.381967\n",
      "Total T: 41018 Episode Num: 5340 Episode T: 3 Reward: -0.643136\n",
      "Total T: 41087 Episode Num: 5360 Episode T: 2 Reward: -0.778595\n",
      "Total T: 41148 Episode Num: 5380 Episode T: 3 Reward: -0.668176\n",
      "Total T: 41223 Episode Num: 5400 Episode T: 5 Reward: -0.915908\n",
      "Total T: 41301 Episode Num: 5420 Episode T: 3 Reward: -0.924296\n",
      "Total T: 41367 Episode Num: 5440 Episode T: 3 Reward: -0.436343\n",
      "Total T: 41436 Episode Num: 5460 Episode T: 3 Reward: -0.515057\n",
      "Total T: 41498 Episode Num: 5480 Episode T: 2 Reward: -0.775448\n",
      "Total T: 41567 Episode Num: 5500 Episode T: 4 Reward: -0.906165\n",
      "Total T: 41635 Episode Num: 5520 Episode T: 4 Reward: -0.056717\n",
      "Total T: 41701 Episode Num: 5540 Episode T: 3 Reward: -1.063109\n",
      "Total T: 41768 Episode Num: 5560 Episode T: 3 Reward: -0.513294\n",
      "Total T: 41838 Episode Num: 5580 Episode T: 3 Reward: -0.254560\n",
      "Total T: 41906 Episode Num: 5600 Episode T: 4 Reward: -0.311370\n",
      "Total T: 41968 Episode Num: 5620 Episode T: 4 Reward: -0.439519\n",
      "Total T: 42031 Episode Num: 5640 Episode T: 3 Reward: 0.109005\n",
      "Total T: 42098 Episode Num: 5660 Episode T: 2 Reward: -0.770845\n",
      "Total T: 42170 Episode Num: 5680 Episode T: 4 Reward: -0.606905\n",
      "Total T: 42239 Episode Num: 5700 Episode T: 3 Reward: 0.513041\n",
      "Total T: 42301 Episode Num: 5720 Episode T: 3 Reward: -0.701824\n",
      "Total T: 42370 Episode Num: 5740 Episode T: 4 Reward: -0.345389\n",
      "Total T: 42439 Episode Num: 5760 Episode T: 3 Reward: 0.616965\n",
      "Total T: 42516 Episode Num: 5780 Episode T: 4 Reward: -0.515960\n",
      "Total T: 42576 Episode Num: 5800 Episode T: 3 Reward: -0.612199\n",
      "Total T: 42648 Episode Num: 5820 Episode T: 3 Reward: 0.031559\n",
      "Total T: 42709 Episode Num: 5840 Episode T: 3 Reward: -0.275907\n",
      "Total T: 42785 Episode Num: 5860 Episode T: 3 Reward: -0.424126\n",
      "Total T: 42852 Episode Num: 5880 Episode T: 2 Reward: -0.765067\n",
      "Total T: 42922 Episode Num: 5900 Episode T: 3 Reward: -0.537602\n",
      "Total T: 42989 Episode Num: 5920 Episode T: 3 Reward: -0.387359\n",
      "Total T: 43057 Episode Num: 5940 Episode T: 4 Reward: -0.754946\n",
      "Total T: 43122 Episode Num: 5960 Episode T: 4 Reward: -0.759143\n",
      "Total T: 43192 Episode Num: 5980 Episode T: 3 Reward: 0.150258\n",
      "Total T: 43264 Episode Num: 6000 Episode T: 3 Reward: 0.705365\n",
      "Total T: 43330 Episode Num: 6020 Episode T: 3 Reward: -0.791804\n",
      "Total T: 43399 Episode Num: 6040 Episode T: 3 Reward: -0.520860\n",
      "Total T: 43483 Episode Num: 6060 Episode T: 4 Reward: -0.545158\n",
      "Total T: 43550 Episode Num: 6080 Episode T: 4 Reward: -0.653590\n",
      "Total T: 43620 Episode Num: 6100 Episode T: 6 Reward: -0.662211\n",
      "Total T: 43685 Episode Num: 6120 Episode T: 3 Reward: -0.773478\n",
      "Total T: 43753 Episode Num: 6140 Episode T: 4 Reward: -0.743898\n",
      "Total T: 43821 Episode Num: 6160 Episode T: 3 Reward: -0.680816\n",
      "Total T: 43884 Episode Num: 6180 Episode T: 3 Reward: -0.021693\n",
      "Total T: 43957 Episode Num: 6200 Episode T: 3 Reward: -0.788971\n",
      "Total T: 44025 Episode Num: 6220 Episode T: 3 Reward: -0.523886\n",
      "Total T: 44090 Episode Num: 6240 Episode T: 3 Reward: -0.658254\n",
      "Total T: 44156 Episode Num: 6260 Episode T: 3 Reward: -0.178903\n",
      "Total T: 44221 Episode Num: 6280 Episode T: 3 Reward: 0.307790\n",
      "Total T: 44291 Episode Num: 6300 Episode T: 3 Reward: -0.604588\n",
      "Total T: 44360 Episode Num: 6320 Episode T: 3 Reward: 0.442845\n",
      "Total T: 44436 Episode Num: 6340 Episode T: 4 Reward: 0.413128\n",
      "Total T: 44504 Episode Num: 6360 Episode T: 3 Reward: -0.617707\n",
      "Total T: 44573 Episode Num: 6380 Episode T: 6 Reward: 0.805893\n",
      "Total T: 44644 Episode Num: 6400 Episode T: 4 Reward: -0.702827\n",
      "Total T: 44708 Episode Num: 6420 Episode T: 5 Reward: -0.353485\n",
      "Total T: 44765 Episode Num: 6440 Episode T: 3 Reward: -0.566190\n",
      "Total T: 44837 Episode Num: 6460 Episode T: 3 Reward: -0.325442\n",
      "Total T: 44908 Episode Num: 6480 Episode T: 3 Reward: 0.012430\n",
      "Total T: 44970 Episode Num: 6500 Episode T: 3 Reward: -0.056922\n",
      "Total T: 45038 Episode Num: 6520 Episode T: 4 Reward: -1.113872\n",
      "---------------------------------------\n",
      "Episode_num: 6541: -0.747703\n",
      "---------------------------------------\n",
      "Total T: 45159 Episode Num: 6560 Episode T: 4 Reward: -0.699517\n",
      "Total T: 45229 Episode Num: 6580 Episode T: 8 Reward: -1.079164\n",
      "Total T: 45292 Episode Num: 6600 Episode T: 4 Reward: -0.328789\n",
      "Total T: 45364 Episode Num: 6620 Episode T: 7 Reward: 0.362321\n",
      "Total T: 45446 Episode Num: 6640 Episode T: 11 Reward: -0.746188\n",
      "Total T: 45513 Episode Num: 6660 Episode T: 3 Reward: 0.302878\n",
      "Total T: 45581 Episode Num: 6680 Episode T: 4 Reward: -0.848044\n",
      "Total T: 45652 Episode Num: 6700 Episode T: 3 Reward: 0.381819\n",
      "Total T: 45730 Episode Num: 6720 Episode T: 3 Reward: -0.194588\n",
      "Total T: 45793 Episode Num: 6740 Episode T: 4 Reward: -0.809923\n",
      "Total T: 45859 Episode Num: 6760 Episode T: 3 Reward: -0.038840\n",
      "Total T: 45929 Episode Num: 6780 Episode T: 3 Reward: 0.334118\n",
      "Total T: 45983 Episode Num: 6800 Episode T: 3 Reward: 0.165424\n",
      "Total T: 46044 Episode Num: 6820 Episode T: 3 Reward: -0.976253\n",
      "Total T: 46118 Episode Num: 6840 Episode T: 2 Reward: -0.739758\n",
      "Total T: 46192 Episode Num: 6860 Episode T: 2 Reward: -0.739173\n",
      "Total T: 46260 Episode Num: 6880 Episode T: 3 Reward: -0.773315\n",
      "Total T: 46328 Episode Num: 6900 Episode T: 3 Reward: -0.914203\n",
      "Total T: 46396 Episode Num: 6920 Episode T: 8 Reward: 439.918336\n",
      "Total T: 46471 Episode Num: 6940 Episode T: 2 Reward: -0.736998\n",
      "Total T: 46542 Episode Num: 6960 Episode T: 7 Reward: 439.669154\n",
      "Total T: 46613 Episode Num: 6980 Episode T: 5 Reward: -0.264645\n",
      "Total T: 46678 Episode Num: 7000 Episode T: 3 Reward: -0.405291\n",
      "Total T: 46745 Episode Num: 7020 Episode T: 3 Reward: -0.907730\n",
      "Total T: 46817 Episode Num: 7040 Episode T: 3 Reward: -0.262191\n",
      "Total T: 46877 Episode Num: 7060 Episode T: 3 Reward: -0.132445\n",
      "Total T: 46938 Episode Num: 7080 Episode T: 3 Reward: -0.710731\n",
      "Total T: 47008 Episode Num: 7100 Episode T: 7 Reward: -0.836675\n",
      "Total T: 47073 Episode Num: 7120 Episode T: 3 Reward: 0.205087\n",
      "Total T: 47139 Episode Num: 7140 Episode T: 4 Reward: -0.926210\n",
      "Total T: 47205 Episode Num: 7160 Episode T: 3 Reward: -0.345747\n",
      "Total T: 47264 Episode Num: 7180 Episode T: 3 Reward: -0.305252\n",
      "Total T: 47329 Episode Num: 7200 Episode T: 3 Reward: 0.483548\n",
      "Total T: 47391 Episode Num: 7220 Episode T: 2 Reward: -0.729814\n",
      "Total T: 47452 Episode Num: 7240 Episode T: 3 Reward: -0.489766\n",
      "Total T: 47527 Episode Num: 7260 Episode T: 5 Reward: -1.112243\n",
      "Total T: 47594 Episode Num: 7280 Episode T: 4 Reward: -0.714900\n",
      "Total T: 47663 Episode Num: 7300 Episode T: 3 Reward: -0.650661\n",
      "Total T: 47731 Episode Num: 7320 Episode T: 3 Reward: -0.171076\n",
      "Total T: 47797 Episode Num: 7340 Episode T: 3 Reward: -0.019720\n",
      "Total T: 47867 Episode Num: 7360 Episode T: 8 Reward: 439.231557\n",
      "Total T: 47932 Episode Num: 7380 Episode T: 2 Reward: -0.725664\n",
      "Total T: 47989 Episode Num: 7400 Episode T: 3 Reward: -1.076960\n",
      "Total T: 48051 Episode Num: 7420 Episode T: 3 Reward: -0.434999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 48114 Episode Num: 7440 Episode T: 3 Reward: -0.696644\n",
      "Total T: 48186 Episode Num: 7460 Episode T: 4 Reward: -0.363317\n",
      "Total T: 48249 Episode Num: 7480 Episode T: 2 Reward: -0.723095\n",
      "Total T: 48304 Episode Num: 7500 Episode T: 3 Reward: -0.167865\n",
      "Total T: 48376 Episode Num: 7520 Episode T: 4 Reward: -0.668306\n",
      "Total T: 48440 Episode Num: 7540 Episode T: 3 Reward: -0.615165\n",
      "Total T: 48503 Episode Num: 7560 Episode T: 3 Reward: -0.125034\n",
      "Total T: 48565 Episode Num: 7580 Episode T: 3 Reward: 0.765571\n",
      "Total T: 48633 Episode Num: 7600 Episode T: 5 Reward: -0.528495\n",
      "Total T: 48696 Episode Num: 7620 Episode T: 2 Reward: -0.734448\n",
      "Total T: 48755 Episode Num: 7640 Episode T: 3 Reward: 0.240622\n",
      "Total T: 48815 Episode Num: 7660 Episode T: 3 Reward: -0.013495\n",
      "Total T: 48876 Episode Num: 7680 Episode T: 5 Reward: 0.217159\n",
      "Total T: 48940 Episode Num: 7700 Episode T: 3 Reward: -0.175599\n",
      "Total T: 48998 Episode Num: 7720 Episode T: 4 Reward: -0.583267\n",
      "Total T: 49057 Episode Num: 7740 Episode T: 3 Reward: -0.436623\n",
      "Total T: 49121 Episode Num: 7760 Episode T: 2 Reward: -0.716248\n",
      "Total T: 49182 Episode Num: 7780 Episode T: 3 Reward: 0.516415\n",
      "Total T: 49259 Episode Num: 7800 Episode T: 3 Reward: -0.626873\n",
      "Total T: 49317 Episode Num: 7820 Episode T: 3 Reward: 0.143109\n",
      "Total T: 49384 Episode Num: 7840 Episode T: 5 Reward: -0.851077\n",
      "Total T: 49444 Episode Num: 7860 Episode T: 3 Reward: 0.402593\n",
      "Total T: 49503 Episode Num: 7880 Episode T: 3 Reward: -0.806706\n",
      "Total T: 49571 Episode Num: 7900 Episode T: 4 Reward: 0.345041\n",
      "Total T: 49636 Episode Num: 7920 Episode T: 3 Reward: -0.133591\n",
      "Total T: 49709 Episode Num: 7940 Episode T: 3 Reward: -1.066021\n",
      "Total T: 49781 Episode Num: 7960 Episode T: 4 Reward: 0.393635\n",
      "Total T: 49841 Episode Num: 7980 Episode T: 3 Reward: 0.105344\n",
      "Total T: 49909 Episode Num: 8000 Episode T: 3 Reward: 0.683521\n",
      "Total T: 49977 Episode Num: 8020 Episode T: 5 Reward: -0.399115\n",
      "Total T: 50040 Episode Num: 8040 Episode T: 4 Reward: -0.278696\n",
      "---------------------------------------\n",
      "Episode_num: 8056: -0.708563\n",
      "---------------------------------------\n",
      "Total T: 50110 Episode Num: 8060 Episode T: 3 Reward: 0.061436\n",
      "Total T: 50176 Episode Num: 8080 Episode T: 4 Reward: -1.067968\n",
      "Total T: 50238 Episode Num: 8100 Episode T: 3 Reward: -0.177839\n",
      "Total T: 50305 Episode Num: 8120 Episode T: 4 Reward: -0.964166\n",
      "Total T: 50369 Episode Num: 8140 Episode T: 2 Reward: -0.706458\n",
      "Total T: 50432 Episode Num: 8160 Episode T: 3 Reward: -0.343097\n",
      "Total T: 50494 Episode Num: 8180 Episode T: 3 Reward: -0.314231\n",
      "Total T: 50557 Episode Num: 8200 Episode T: 3 Reward: 0.520542\n",
      "Total T: 50620 Episode Num: 8220 Episode T: 2 Reward: -0.704437\n",
      "Total T: 50686 Episode Num: 8240 Episode T: 4 Reward: -0.013606\n",
      "Total T: 50750 Episode Num: 8260 Episode T: 5 Reward: -0.953061\n",
      "Total T: 50825 Episode Num: 8280 Episode T: 4 Reward: 0.102264\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d8e12937ae1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta_start\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbeta_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             policy.train(replay_buffer, episode_timesteps, beta, args.batch_size, \n\u001b[0;32m--> 101\u001b[0;31m                              args.discount, args.tau, args.policy_noise, args.noise_clip, args.policy_freq)\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mReward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/RL/needle_master_tools/DDPG_TD3/TD3_priorized.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, iterations, beta_PER, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_priorities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprios\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;31m# Delayed policy updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s\" % (args.filename, args.policy_name)\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "if not os.path.exists(\"./results\"):\n",
    "    os.makedirs(\"./results\")\n",
    "if not os.path.exists(\"./pytorch_models\"):\n",
    "    os.makedirs(\"./pytorch_models\")\n",
    "\n",
    "## environment set up\n",
    "action_dim = 2\n",
    "\n",
    "\"\"\" Adding the log file \"\"\"\n",
    "logfile = \"%s_%s\" % (args.filename, args.policy_name)\n",
    "log_f = open(\"log_\"+logfile+\".txt\",\"w+\")\n",
    "env_path = '/home/lifan/workspace/RL/needle_master_tools/data/'+ args.filename + '.txt'\n",
    "env = Environment(action_dim,log_f, filename = env_path)\n",
    "\n",
    "state_dim = len(env.gates) + 9\n",
    "\n",
    "\n",
    "\"\"\"\"  for PID controller \"\"\"\n",
    "action_constrain = [10, np.pi/20]\n",
    "parameter = [0.1,0.0009]\n",
    "pid = PID( parameter, env.width, env.height )\n",
    "\n",
    "\"\"\" [lower bound],[higher bound] \"\"\"\n",
    "# env.action_bound = np.array((-1,1)) ## for one dimension action\n",
    "env.action_bound = np.array(([0, -1],[1, 1]))   ## for two dimension action\n",
    "max_action = 1\n",
    "\n",
    "\n",
    "\"\"\" parameters for epsilon declay \"\"\"\n",
    "epsilon_start = 1\n",
    "epsilon_final = 0.01\n",
    "decay_rate = 250000\n",
    "ep_decay = []\n",
    "\n",
    "\"\"\" beta Prioritized Experience Replay\"\"\"\n",
    "beta_start = 0.4\n",
    "beta_frames = 250000\n",
    "\n",
    "\n",
    "### for plotting\n",
    "Reward = []\n",
    "save_path = './out/'\n",
    "\"\"\" start straightly \"\"\"\n",
    "evaluations = []\n",
    "\n",
    "# Initialize policy\n",
    "# policy = TD3.TD3(state_dim, action_dim, max_action)\n",
    "policy = TD3_priorized.TD3(state_dim, action_dim, max_action)\n",
    "# replay_buffer = utils.ReplayBuffer(args.max_size)\n",
    "replay_buffer = utils.NaivePrioritizedBuffer(int(args.max_size))\n",
    "\n",
    "# Evaluate untrained policy\n",
    "# evaluations = [evaluate_policy(policy)]\n",
    "\n",
    "\n",
    "env.total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "done = True\n",
    "\n",
    "while env.total_timesteps < args.max_timesteps:\n",
    "\n",
    "    # Evaluate episode\n",
    "    if timesteps_since_eval >= args.eval_freq:\n",
    "        timesteps_since_eval %= args.eval_freq\n",
    "        evaluations.append(evaluate_policy(policy, log_f))\n",
    "        \n",
    "        if env.last_reward > 100 and env.episode_num > 100: \n",
    "            policy.save(file_name, directory=\"./pytorch_models\")\n",
    "            np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "\n",
    "        continue\n",
    "\n",
    "\n",
    "    ## finish one episode, and train episode_times\n",
    "    if done:\n",
    "#         log_f.write('~~~~~~~~~~~~~~~~~~~~~~~~ iteration {} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n'.format(env.episode_num))\n",
    "\n",
    "\n",
    "        ## load model\n",
    "        # policy.load(file_name,\"./pytorch_models\")\n",
    "\n",
    "        ## training as usual\n",
    "            # if env.total_timesteps != 0 and env.episode_reward > 500:\n",
    "        if env.total_timesteps != 0:\n",
    "            log_f.write('Total:{}, Episode Num:{}, Eposide:{}, Reward:{}\\n'.format(env.total_timesteps, env.episode_num, episode_timesteps, env.episode_reward))\n",
    "            log_f.flush()\n",
    "            \n",
    "            if env.episode_num % 20 == 0:\n",
    "                print ((\"Total T: %d Episode Num: %d Episode T: %d Reward: %f\") % (\n",
    "                env.total_timesteps, env.episode_num, episode_timesteps, env.episode_reward))\n",
    "                env.render( save_image=True, save_path=save_path)\n",
    "\n",
    "        if env.total_timesteps != 0:\n",
    "            beta = min(1.0, beta_start + env.total_timesteps * (1.0 - beta_start) / beta_frames)\n",
    "            policy.train(replay_buffer, episode_timesteps, beta, args.batch_size, \n",
    "                             args.discount, args.tau, args.policy_noise, args.noise_clip, args.policy_freq)\n",
    "\n",
    "        Reward.append(env.episode_reward)\n",
    "\n",
    "        # Reset environment\n",
    "        state = env.reset(log_f)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        env.episode_num += 1\n",
    "        env.episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "\n",
    "    \"\"\" exploration rate decay \"\"\"\n",
    "    args.expl_noise = (epsilon_start - epsilon_final) * math.exp(-1. * env.total_timesteps / decay_rate)\n",
    "    ep_decay.append(args.expl_noise)\n",
    "#     log_f.write('epsilon decay:{}\\n'.format(args.expl_noise))\n",
    "#     if env.total_timesteps % 500 == 0 and args.expl_noise > 0:\n",
    "#         args.expl_noise *= 0.9\n",
    "\n",
    "    \"\"\" alternative between random selected action and policy selected action \"\"\"\n",
    "#     if env.total_timesteps % args.pid_freq < args.pid_interval:\n",
    "# #     if env.total_timesteps < args.pid_interval:\n",
    "#         state_pid = state[0:3]\n",
    "#         action = pid.PIDcontroller( state_pid, env.next_gate, env.gates)\n",
    "# #         log_f.write('PID Action:{}\\n'.format(action))\n",
    "# #         action = env.sample_action()\n",
    "#         # log_f.write('~~~~~~~~~~~random action~~~~~~~~~~\\n')\n",
    "#         # log_f.write('random selected action:{}\\n'.format(action))\n",
    "\n",
    "#     else:\n",
    "#         # print(\"state: \" +str(state))\n",
    "#         action = policy.select_action(state)\n",
    "#         # print(\"select\")\n",
    "#         # log_f.write('~~~~~~~~~~~selected action~~~~~~~~~~\\n')\n",
    "#         log_f.write('Action based on policy:{}\\n'.format(action))\n",
    "#         # print(\"action based on policy:\" + str(action))\n",
    "#         # print(\"action selected: \" +str(action))\n",
    "        \n",
    "#         if args.expl_noise != 0:\n",
    "#             noise = np.random.normal(0, args.expl_noise, size=action_dim)\n",
    "#             # print(\"noise: \" + str(noise))\n",
    "#             action = (action + noise).clip(-1, 1)\n",
    "\n",
    "\n",
    "    \"\"\" using PID controller \"\"\"\n",
    "    # state_pid = state[0:3]\n",
    "    # action = pid.PIDcontroller( state_pid, env.next_gate, env.gates)\n",
    "    # print(\"action based on PID: \" + str(action))\n",
    "\n",
    "    \"\"\" action selected based on pure policy \"\"\"\n",
    "    action = policy.select_action(state)\n",
    "    log_f.write('action based on policy:{}\\n'.format(action))\n",
    "    # print(\"action based on policy:\" + str(action))\n",
    "    if args.expl_noise != 0:\n",
    "#         state_pid = state[0:3]\n",
    "#         guidance = pid.PIDcontroller( state_pid, env.next_gate, env.gates, env.total_timesteps)\n",
    "        noise = np.random.normal(0, args.expl_noise, size=action_dim)\n",
    "        # print(\"noise: \" + str(noise))\n",
    "#         action = ((1 - args.expl_noise) * action + args.expl_noise * guidance)\n",
    "        action = action + noise\n",
    "        action[0] = np.clip(action[0],0,1)\n",
    "        action[1] = np.clip(action[1],-1,1)\n",
    "\n",
    "\n",
    "    ### select action only based on pure RL\n",
    "    # action = policy.select_action(state)\n",
    "    # print(\"action selected: \" +str(action))\n",
    "\n",
    "\n",
    "    # Perform action\n",
    "    new_state, reward, done = env.step(action, log_f)\n",
    "\n",
    "    done_bool = 0 if episode_timesteps + 1 == env.max_time else float(done)\n",
    "    env.episode_reward += reward\n",
    "\n",
    "    # Store data in replay buffer\n",
    "    replay_buffer.add(state, new_state, action, reward, done_bool)\n",
    "    # print(\"state: \" + str(state))\n",
    "    state = new_state\n",
    "\n",
    "    episode_timesteps += 1\n",
    "    env.total_timesteps += 1\n",
    "    timesteps_since_eval += 1\n",
    "\n",
    "plt.plot(range(len(Reward)), np.array(Reward), 'b')\n",
    "plt.savefig('./results/episode reward.png')\n",
    "\n",
    "plt.plot(range(len(policy.actor_loss)), policy.actor_loss, 'b')\n",
    "plt.savefig('./results/actor loss.png')\n",
    "\n",
    "plt.plot(range(len(policy.critic_loss)), policy.critic_loss, 'b')\n",
    "plt.savefig('./results/critic loss.png')\n",
    "\n",
    "plt.plot(range(len(evaluations)), np.array(evaluations), 'b')\n",
    "plt.savefig('./results/evaluation reward.png')\n",
    "print(evaluations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
