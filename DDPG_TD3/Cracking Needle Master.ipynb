{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sloving Needle Master with Twin Delayed DDPG (TD3)\n",
    "Code modified from https://github.com/nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from environment import Environment\n",
    "from environment import PID\n",
    "import utils\n",
    "import TD3_priorized\n",
    "import TD3\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    policy_name = \"TD3\"\n",
    "    env_name = \"Needle Master\"\n",
    "    seed = 1e6\n",
    "    eval_freq = 5e3 # How often (time steps) we evaluate\n",
    "    max_timesteps = 1e5  # Max time steps to run environment for\n",
    "    save_models = \"store\"\n",
    "    expl_noise = 1    # Std of Gaussian exploration noise\n",
    "    batch_size = 100\n",
    "    discount = 0.99   # Discount factor\n",
    "    tau = 0.005         # Target network update rate\n",
    "    policy_noise = 0.2   # Noise added to target policy during critic update\n",
    "    noise_clip = 0.5\n",
    "    policy_freq = 2  # Frequency of delayed policy updates\n",
    "    max_size = 1e6\n",
    "    pid_freq = 9e2    # How often purely random policy is run for\n",
    "    pid_interval = 5e2   # How many time steps purely random policy is run for\n",
    "    filename = 'environment_17'\n",
    "    \n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(random.randint(1, 10000))\n",
    "if torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed(random.randint(1, 10000))\n",
    "    torch.backends.cudnn.enabled = False  # Disable nondeterministic ops (not sure if critical but better safe than sorry)\n",
    "else:\n",
    "    args.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/home/lifan/workspace/RL/needle_master_tools/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, log_f):\n",
    "    eval_path = './evaluate/'\n",
    "    evaluation_time = 3\n",
    "    if not os.path.exists(eval_path):\n",
    "        os.mkdir(eval_path)\n",
    "\n",
    "    state = env.reset(log_f)\n",
    "    done = False\n",
    "    env.episode_num += 1\n",
    "    env.episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    average_reward = 0\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        action = policy.select_action(state)\n",
    "        # print(\"state: \" + str(state))\n",
    "        # print(\"action: \" + str(action))\n",
    "        new_state, reward, done = env.step(action, log_f)\n",
    "        # print(\"next state: \" + str(next_state))\n",
    "        # print(\"done: \" +str(done))\n",
    "        env.episode_reward += reward\n",
    "        state = new_state\n",
    "        episode_timesteps += 1\n",
    "        env.total_timesteps += 1\n",
    "\n",
    "    env.render(save_image=True, save_path=eval_path)\n",
    "\n",
    "    print (\"---------------------------------------\")\n",
    "    print (\"Episode_num: %d: %f\" % (env.episode_num, env.episode_reward))\n",
    "    print (\"---------------------------------------\")\n",
    "    return env.episode_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"%s_%s\" % (args.filename, args.policy_name)\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "if not os.path.exists(\"./results\"):\n",
    "    os.makedirs(\"./results\")\n",
    "if not os.path.exists(\"./pytorch_models\"):\n",
    "    os.makedirs(\"./pytorch_models\")\n",
    "\n",
    "## environment set up\n",
    "action_dim = 2\n",
    "\n",
    "\"\"\" Adding the log file \"\"\"\n",
    "logfile = \"%s_%s\" % (args.filename, args.policy_name)\n",
    "log_f = open(\"log_\"+logfile+\".txt\",\"w+\")\n",
    "env_path = '/home/lifan/workspace/RL/needle_master_tools/data/'+ args.filename + '.txt'\n",
    "env = Environment(action_dim,log_f, filename = env_path)\n",
    "\n",
    "state_dim = len(env.gates) + 9\n",
    "\n",
    "\n",
    "\"\"\"\"  for PID controller \"\"\"\n",
    "action_constrain = [10, np.pi/20]\n",
    "parameter = [0.1,0.0009]\n",
    "pid = PID( parameter, env.width, env.height )\n",
    "\n",
    "\"\"\" [lower bound],[higher bound] \"\"\"\n",
    "# env.action_bound = np.array((-1,1)) ## for one dimension action\n",
    "env.action_bound = np.array(([0, -1],[1, 1]))   ## for two dimension action\n",
    "max_action = 1\n",
    "\n",
    "\n",
    "\"\"\" parameters for epsilon declay \"\"\"\n",
    "epsilon_start = 1\n",
    "epsilon_final = 0.01\n",
    "decay_rate = 25000\n",
    "ep_decay = []\n",
    "\n",
    "\"\"\" beta Prioritized Experience Replay\"\"\"\n",
    "beta_start = 0.4\n",
    "beta_frames = 25000\n",
    "\n",
    "\n",
    "### for plotting\n",
    "Reward = []\n",
    "save_path = './out/'\n",
    "\"\"\" start straightly \"\"\"\n",
    "evaluations = []\n",
    "\n",
    "# Initialize policy\n",
    "# policy = TD3.TD3(state_dim, action_dim, max_action)\n",
    "policy = TD3_priorized.TD3(state_dim, action_dim, max_action)\n",
    "# replay_buffer = utils.ReplayBuffer(args.max_size)\n",
    "replay_buffer = utils.NaivePrioritizedBuffer(int(args.max_size))\n",
    "\n",
    "# Evaluate untrained policy\n",
    "# evaluations = [evaluate_policy(policy)]\n",
    "\n",
    "\n",
    "env.total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "done = True\n",
    "\n",
    "while env.total_timesteps < args.max_timesteps:\n",
    "\n",
    "    # Evaluate episode\n",
    "    if timesteps_since_eval >= args.eval_freq:\n",
    "        timesteps_since_eval %= args.eval_freq\n",
    "        evaluations.append(evaluate_policy(policy, log_f))\n",
    "        \n",
    "        if env.last_reward > 100 and env.episode_num > 100: \n",
    "            policy.save(file_name, directory=\"./pytorch_models\")\n",
    "            np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "\n",
    "        continue\n",
    "\n",
    "\n",
    "    ## finish one episode, and train episode_times\n",
    "    if done:\n",
    "#         log_f.write('~~~~~~~~~~~~~~~~~~~~~~~~ iteration {} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n'.format(env.episode_num))\n",
    "\n",
    "\n",
    "        ## load model\n",
    "        # policy.load(file_name,\"./pytorch_models\")\n",
    "\n",
    "        ## training as usual\n",
    "            # if env.total_timesteps != 0 and env.episode_reward > 500:\n",
    "        if env.total_timesteps != 0:\n",
    "            log_f.write('Total:{}, Episode Num:{}, Eposide:{}, Reward:{}\\n'.format(env.total_timesteps, env.episode_num, episode_timesteps, env.episode_reward))\n",
    "            log_f.flush()\n",
    "            \n",
    "            if env.episode_num % 1 == 0:\n",
    "                print ((\"Total T: %d Episode Num: %d Episode T: %d Reward: %f\") % (\n",
    "                env.total_timesteps, env.episode_num, episode_timesteps, env.episode_reward))\n",
    "                env.render( save_image=True, save_path=save_path)\n",
    "\n",
    "        if env.total_timesteps != 0:\n",
    "            beta = min(1.0, beta_start + env.total_timesteps * (1.0 - beta_start) / beta_frames)\n",
    "            policy.train(replay_buffer, episode_timesteps, beta, args.batch_size, \n",
    "                             args.discount, args.tau, args.policy_noise, args.noise_clip, args.policy_freq)\n",
    "\n",
    "        Reward.append(env.episode_reward)\n",
    "\n",
    "        # Reset environment\n",
    "        state = env.reset(log_f)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        env.episode_num += 1\n",
    "        env.episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "\n",
    "    \"\"\" exploration rate decay \"\"\"\n",
    "    args.expl_noise = (epsilon_start - epsilon_final) * math.exp(-1. * env.total_timesteps / decay_rate)\n",
    "    ep_decay.append(args.expl_noise)\n",
    "#     log_f.write('epsilon decay:{}\\n'.format(args.expl_noise))\n",
    "#     if env.total_timesteps % 500 == 0 and args.expl_noise > 0:\n",
    "#         args.expl_noise *= 0.9\n",
    "\n",
    "    \"\"\" alternative between random selected action and policy selected action \"\"\"\n",
    "#     if env.total_timesteps % args.pid_freq < args.pid_interval:\n",
    "# #     if env.total_timesteps < args.pid_interval:\n",
    "#         state_pid = state[0:3]\n",
    "#         action = pid.PIDcontroller( state_pid, env.next_gate, env.gates)\n",
    "# #         log_f.write('PID Action:{}\\n'.format(action))\n",
    "# #         action = env.sample_action()\n",
    "#         # log_f.write('~~~~~~~~~~~random action~~~~~~~~~~\\n')\n",
    "#         # log_f.write('random selected action:{}\\n'.format(action))\n",
    "\n",
    "#     else:\n",
    "#         # print(\"state: \" +str(state))\n",
    "#         action = policy.select_action(state)\n",
    "#         # print(\"select\")\n",
    "#         # log_f.write('~~~~~~~~~~~selected action~~~~~~~~~~\\n')\n",
    "#         log_f.write('Action based on policy:{}\\n'.format(action))\n",
    "#         # print(\"action based on policy:\" + str(action))\n",
    "#         # print(\"action selected: \" +str(action))\n",
    "        \n",
    "#         if args.expl_noise != 0:\n",
    "#             noise = np.random.normal(0, args.expl_noise, size=action_dim)\n",
    "#             # print(\"noise: \" + str(noise))\n",
    "#             action = (action + noise).clip(-1, 1)\n",
    "\n",
    "\n",
    "    \"\"\" using PID controller \"\"\"\n",
    "    # state_pid = state[0:3]\n",
    "    # action = pid.PIDcontroller( state_pid, env.next_gate, env.gates)\n",
    "    # print(\"action based on PID: \" + str(action))\n",
    "\n",
    "    \"\"\" action selected based on pure policy \"\"\"\n",
    "    action = policy.select_action(state)\n",
    "    log_f.write('action based on policy:{}\\n'.format(action))\n",
    "    # print(\"action based on policy:\" + str(action))\n",
    "    if args.expl_noise != 0:\n",
    "#         state_pid = state[0:3]\n",
    "#         guidance = pid.PIDcontroller( state_pid, env.next_gate, env.gates, env.total_timesteps)\n",
    "        noise = np.random.normal(0, args.expl_noise, size=action_dim)\n",
    "        # print(\"noise: \" + str(noise))\n",
    "#         action = ((1 - args.expl_noise) * action + args.expl_noise * guidance)\n",
    "        action = action + noise\n",
    "        action[0] = np.clip(action[0],0,1)\n",
    "        action[1] = np.clip(action[1],-1,1)\n",
    "\n",
    "\n",
    "    ### select action only based on pure RL\n",
    "    # action = policy.select_action(state)\n",
    "    # print(\"action selected: \" +str(action))\n",
    "\n",
    "\n",
    "    # Perform action\n",
    "    new_state, reward, done = env.step(action, log_f)\n",
    "\n",
    "    done_bool = 0 if episode_timesteps + 1 == env.max_time else float(done)\n",
    "    env.episode_reward += reward\n",
    "\n",
    "    # Store data in replay buffer\n",
    "    replay_buffer.add(state, new_state, action, reward, done_bool)\n",
    "    # print(\"state: \" + str(state))\n",
    "    state = new_state\n",
    "\n",
    "    episode_timesteps += 1\n",
    "    env.total_timesteps += 1\n",
    "    timesteps_since_eval += 1\n",
    "\n",
    "plt.plot(range(len(Reward)), np.array(Reward), 'b')\n",
    "plt.savefig('./results/episode reward.png')\n",
    "\n",
    "plt.plot(range(len(policy.actor_loss)), policy.actor_loss, 'b')\n",
    "plt.savefig('./results/actor loss.png')\n",
    "\n",
    "plt.plot(range(len(policy.critic_loss)), policy.critic_loss, 'b')\n",
    "plt.savefig('./results/critic loss.png')\n",
    "\n",
    "plt.plot(range(len(evaluations)), np.array(evaluations), 'b')\n",
    "plt.savefig('./results/evaluation reward.png')\n",
    "print(evaluations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
