{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sloving Needle Master with Twin Delayed DDPG (TD3)\n",
    "Code modified from https://github.com/nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from environment import Environment\n",
    "from environment import PID\n",
    "import utils\n",
    "import TD3_priorized\n",
    "import TD3\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    policy_name = \"TD3\"\n",
    "    env_name = \"Needle Master\"\n",
    "    seed = 1e6\n",
    "    eval_freq = 5e3 # How often (time steps) we evaluate\n",
    "    max_timesteps = 1e5  # Max time steps to run environment for\n",
    "    save_models = \"store\"\n",
    "    expl_noise = 1    # Std of Gaussian exploration noise\n",
    "    batch_size = 100\n",
    "    discount = 0.99   # Discount factor\n",
    "    tau = 0.005         # Target network update rate\n",
    "    policy_noise = 0.2   # Noise added to target policy during critic update\n",
    "    noise_clip = 0.5\n",
    "    policy_freq = 2  # Frequency of delayed policy updates\n",
    "    max_size = 1e6\n",
    "    pid_freq = 9e2    # How often purely random policy is run for\n",
    "    pid_interval = 5e2   # How many time steps purely random policy is run for\n",
    "    filename = 'environment_14'\n",
    "    \n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(random.randint(1, 10000))\n",
    "if torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed(random.randint(1, 10000))\n",
    "    torch.backends.cudnn.enabled = False  # Disable nondeterministic ops (not sure if critical but better safe than sorry)\n",
    "else:\n",
    "    args.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/home/lifan/workspace/RL/needle_master_tools/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, log_f):\n",
    "    eval_path = './evaluate/'\n",
    "    evaluation_time = 3\n",
    "    if not os.path.exists(eval_path):\n",
    "        os.mkdir(eval_path)\n",
    "\n",
    "    state = env.reset(log_f)\n",
    "    done = False\n",
    "    env.episode_num += 1\n",
    "    env.episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    average_reward = 0\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        action = policy.select_action(state)\n",
    "        # print(\"state: \" + str(state))\n",
    "        # print(\"action: \" + str(action))\n",
    "        new_state, reward, done = env.step(action, log_f)\n",
    "        # print(\"next state: \" + str(next_state))\n",
    "        # print(\"done: \" +str(done))\n",
    "        env.episode_reward += reward\n",
    "        state = new_state\n",
    "        episode_timesteps += 1\n",
    "        env.total_timesteps += 1\n",
    "\n",
    "    env.render(save_image=True, save_path=eval_path)\n",
    "\n",
    "    print (\"---------------------------------------\")\n",
    "    print (\"Episode_num: %d: %f\" % (env.episode_num, env.episode_reward))\n",
    "    print (\"---------------------------------------\")\n",
    "    return env.episode_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: environment_14_TD3\n",
      "---------------------------------------\n",
      "Total T: 239 Episode Num: 20 Episode T: 14 Reward: -34.650910\n",
      "Total T: 471 Episode Num: 40 Episode T: 9 Reward: 139.236390\n",
      "Total T: 701 Episode Num: 60 Episode T: 9 Reward: 161.448313\n",
      "Total T: 933 Episode Num: 80 Episode T: 16 Reward: 340.917233\n",
      "Total T: 1140 Episode Num: 100 Episode T: 9 Reward: 162.025199\n",
      "Total T: 1323 Episode Num: 120 Episode T: 9 Reward: 161.737937\n",
      "Total T: 1527 Episode Num: 140 Episode T: 9 Reward: 117.959594\n",
      "Total T: 1735 Episode Num: 160 Episode T: 9 Reward: 161.224987\n",
      "Total T: 1949 Episode Num: 180 Episode T: 10 Reward: 147.449884\n",
      "Total T: 2155 Episode Num: 200 Episode T: 9 Reward: -285.400968\n",
      "Total T: 2380 Episode Num: 220 Episode T: 9 Reward: 161.913216\n",
      "Total T: 2593 Episode Num: 240 Episode T: 9 Reward: 301.436599\n",
      "Total T: 2803 Episode Num: 260 Episode T: 10 Reward: 161.478993\n",
      "Total T: 3018 Episode Num: 280 Episode T: 10 Reward: 161.678523\n",
      "Total T: 3216 Episode Num: 300 Episode T: 9 Reward: 140.424734\n",
      "Total T: 3424 Episode Num: 320 Episode T: 16 Reward: 347.009891\n",
      "Total T: 3595 Episode Num: 340 Episode T: 9 Reward: 159.790222\n",
      "Total T: 3777 Episode Num: 360 Episode T: 9 Reward: -203.801136\n",
      "Total T: 3950 Episode Num: 380 Episode T: 8 Reward: 160.781966\n",
      "Total T: 4132 Episode Num: 400 Episode T: 12 Reward: 897.970175\n",
      "Total T: 4305 Episode Num: 420 Episode T: 9 Reward: 160.452273\n",
      "Total T: 4487 Episode Num: 440 Episode T: 9 Reward: 100.401156\n",
      "Total T: 4670 Episode Num: 460 Episode T: 10 Reward: 43.487619\n",
      "Total T: 4859 Episode Num: 480 Episode T: 9 Reward: 701.008213\n",
      "---------------------------------------\n",
      "Episode_num: 497: 898.137539\n",
      "---------------------------------------\n",
      "Total T: 5037 Episode Num: 500 Episode T: 8 Reward: 161.159169\n",
      "Total T: 5235 Episode Num: 520 Episode T: 12 Reward: 109.501546\n",
      "Total T: 5448 Episode Num: 540 Episode T: 11 Reward: 706.185229\n",
      "Total T: 5645 Episode Num: 560 Episode T: 11 Reward: 499.951628\n",
      "Total T: 5864 Episode Num: 580 Episode T: 16 Reward: 498.468359\n",
      "Total T: 6080 Episode Num: 600 Episode T: 15 Reward: 898.134822\n",
      "Total T: 6294 Episode Num: 620 Episode T: 9 Reward: -204.267816\n",
      "Total T: 6486 Episode Num: 640 Episode T: 9 Reward: 91.421072\n",
      "Total T: 6696 Episode Num: 660 Episode T: 9 Reward: 301.474564\n",
      "Total T: 6902 Episode Num: 680 Episode T: 13 Reward: 498.664377\n",
      "Total T: 7128 Episode Num: 700 Episode T: 13 Reward: 898.296522\n",
      "Total T: 7336 Episode Num: 720 Episode T: 12 Reward: 768.262643\n",
      "Total T: 7563 Episode Num: 740 Episode T: 9 Reward: -238.559486\n",
      "Total T: 7785 Episode Num: 760 Episode T: 11 Reward: 357.980328\n",
      "Total T: 8013 Episode Num: 780 Episode T: 12 Reward: 358.097597\n",
      "Total T: 8242 Episode Num: 800 Episode T: 9 Reward: 82.094694\n",
      "Total T: 8479 Episode Num: 820 Episode T: 12 Reward: 358.188340\n",
      "Total T: 8717 Episode Num: 840 Episode T: 11 Reward: 747.619446\n",
      "Total T: 8957 Episode Num: 860 Episode T: 13 Reward: 358.634925\n",
      "Total T: 9191 Episode Num: 880 Episode T: 12 Reward: 358.301392\n",
      "Total T: 9438 Episode Num: 900 Episode T: 10 Reward: -6.129687\n",
      "Total T: 9684 Episode Num: 920 Episode T: 11 Reward: 358.232373\n",
      "Total T: 9929 Episode Num: 940 Episode T: 8 Reward: 700.571996\n",
      "---------------------------------------\n",
      "Episode_num: 948: 358.354475\n",
      "---------------------------------------\n",
      "Total T: 10166 Episode Num: 960 Episode T: 12 Reward: 359.045073\n",
      "Total T: 10421 Episode Num: 980 Episode T: 11 Reward: 358.103281\n",
      "Total T: 10651 Episode Num: 1000 Episode T: 15 Reward: 898.704001\n",
      "Total T: 10900 Episode Num: 1020 Episode T: 10 Reward: 358.003048\n",
      "Total T: 11152 Episode Num: 1040 Episode T: 11 Reward: 358.096468\n",
      "Total T: 11391 Episode Num: 1060 Episode T: 12 Reward: 358.985229\n",
      "Total T: 11646 Episode Num: 1080 Episode T: 13 Reward: 359.139838\n",
      "Total T: 11888 Episode Num: 1100 Episode T: 12 Reward: 161.218593\n",
      "Total T: 12120 Episode Num: 1120 Episode T: 9 Reward: 357.997613\n",
      "Total T: 12371 Episode Num: 1140 Episode T: 14 Reward: 898.966189\n",
      "Total T: 12618 Episode Num: 1160 Episode T: 9 Reward: 161.211791\n",
      "Total T: 12867 Episode Num: 1180 Episode T: 12 Reward: 358.257258\n",
      "Total T: 13094 Episode Num: 1200 Episode T: 10 Reward: 358.115936\n",
      "Total T: 13311 Episode Num: 1220 Episode T: 12 Reward: 358.000682\n",
      "Total T: 13531 Episode Num: 1240 Episode T: 15 Reward: 898.809785\n",
      "Total T: 13753 Episode Num: 1260 Episode T: 10 Reward: 359.171231\n",
      "Total T: 13968 Episode Num: 1280 Episode T: 10 Reward: 358.498550\n",
      "Total T: 14311 Episode Num: 1300 Episode T: 11 Reward: 357.988330\n",
      "Total T: 14522 Episode Num: 1320 Episode T: 11 Reward: 358.405350\n",
      "Total T: 14845 Episode Num: 1340 Episode T: 34 Reward: 441.286415\n",
      "---------------------------------------\n",
      "Episode_num: 1357: 358.189636\n",
      "---------------------------------------\n",
      "Total T: 15107 Episode Num: 1360 Episode T: 10 Reward: 359.234092\n",
      "Total T: 15431 Episode Num: 1380 Episode T: 9 Reward: 358.772598\n",
      "Total T: 15691 Episode Num: 1400 Episode T: 9 Reward: 358.069032\n",
      "Total T: 15933 Episode Num: 1420 Episode T: 11 Reward: 358.885888\n",
      "Total T: 16130 Episode Num: 1440 Episode T: 8 Reward: 358.389891\n",
      "Total T: 16589 Episode Num: 1460 Episode T: 143 Reward: -4121.256386\n",
      "Total T: 16814 Episode Num: 1480 Episode T: 10 Reward: 358.146578\n",
      "Total T: 17007 Episode Num: 1500 Episode T: 9 Reward: -1.656846\n",
      "Total T: 17291 Episode Num: 1520 Episode T: 8 Reward: -1.853667\n",
      "Total T: 17582 Episode Num: 1540 Episode T: 11 Reward: 359.841492\n",
      "Total T: 17819 Episode Num: 1560 Episode T: 9 Reward: -41.546482\n",
      "Total T: 18007 Episode Num: 1580 Episode T: 8 Reward: 358.202118\n",
      "Total T: 18190 Episode Num: 1600 Episode T: 9 Reward: 358.118458\n",
      "Total T: 18517 Episode Num: 1620 Episode T: 10 Reward: 358.603850\n",
      "Total T: 18761 Episode Num: 1640 Episode T: 9 Reward: -1.086407\n",
      "Total T: 18937 Episode Num: 1660 Episode T: 10 Reward: 358.116500\n",
      "Total T: 19124 Episode Num: 1680 Episode T: 11 Reward: 358.013809\n",
      "Total T: 19358 Episode Num: 1700 Episode T: 9 Reward: 358.854389\n",
      "Total T: 19561 Episode Num: 1720 Episode T: 8 Reward: 358.304053\n",
      "Total T: 19788 Episode Num: 1740 Episode T: 40 Reward: 168.492900\n",
      "Total T: 20013 Episode Num: 1760 Episode T: 28 Reward: 329.658402\n",
      "---------------------------------------\n",
      "Episode_num: 1763: 358.175327\n",
      "---------------------------------------\n",
      "Total T: 20240 Episode Num: 1780 Episode T: 10 Reward: 358.169803\n",
      "Total T: 20433 Episode Num: 1800 Episode T: 11 Reward: 360.031429\n",
      "Total T: 20607 Episode Num: 1820 Episode T: 9 Reward: 359.079976\n",
      "Total T: 20789 Episode Num: 1840 Episode T: 9 Reward: 358.036654\n",
      "Total T: 21027 Episode Num: 1860 Episode T: 10 Reward: 359.404286\n",
      "Total T: 21358 Episode Num: 1880 Episode T: 9 Reward: 358.574189\n",
      "Total T: 21695 Episode Num: 1900 Episode T: 37 Reward: 159.472875\n",
      "Total T: 21888 Episode Num: 1920 Episode T: 8 Reward: 358.632925\n",
      "Total T: 22181 Episode Num: 1940 Episode T: 9 Reward: 358.764163\n",
      "Total T: 22416 Episode Num: 1960 Episode T: 7 Reward: -1.870591\n",
      "Total T: 22742 Episode Num: 1980 Episode T: 24 Reward: 339.469544\n",
      "Total T: 23090 Episode Num: 2000 Episode T: 7 Reward: -0.845963\n",
      "Total T: 23424 Episode Num: 2020 Episode T: 8 Reward: -1.749132\n",
      "Total T: 23579 Episode Num: 2040 Episode T: 7 Reward: -0.896468\n",
      "Total T: 23757 Episode Num: 2060 Episode T: 8 Reward: -1.869550\n",
      "Total T: 23928 Episode Num: 2080 Episode T: 7 Reward: -2.015543\n",
      "Total T: 24113 Episode Num: 2100 Episode T: 8 Reward: -0.929667\n",
      "Total T: 24273 Episode Num: 2120 Episode T: 8 Reward: -1.449200\n",
      "Total T: 24433 Episode Num: 2140 Episode T: 10 Reward: 360.067322\n",
      "Total T: 24734 Episode Num: 2160 Episode T: 8 Reward: -0.625093\n",
      "Total T: 24903 Episode Num: 2180 Episode T: 8 Reward: -1.864764\n",
      "---------------------------------------\n",
      "Episode_num: 2199: -41.366459\n",
      "---------------------------------------\n",
      "Total T: 25052 Episode Num: 2200 Episode T: 9 Reward: 359.330100\n",
      "Total T: 25332 Episode Num: 2220 Episode T: 8 Reward: -41.313647\n",
      "Total T: 25598 Episode Num: 2240 Episode T: 11 Reward: 359.198451\n",
      "Total T: 25763 Episode Num: 2260 Episode T: 7 Reward: -1.366828\n",
      "Total T: 26012 Episode Num: 2280 Episode T: 7 Reward: -1.834058\n",
      "Total T: 26235 Episode Num: 2300 Episode T: 7 Reward: -41.531532\n",
      "Total T: 26535 Episode Num: 2320 Episode T: 9 Reward: -40.820733\n",
      "Total T: 26862 Episode Num: 2340 Episode T: 23 Reward: 320.155767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 27122 Episode Num: 2360 Episode T: 9 Reward: 358.870738\n",
      "Total T: 27551 Episode Num: 2380 Episode T: 149 Reward: -6281.362469\n",
      "Total T: 27802 Episode Num: 2400 Episode T: 7 Reward: -1.232858\n",
      "Total T: 27947 Episode Num: 2420 Episode T: 8 Reward: -41.602630\n",
      "Total T: 28158 Episode Num: 2440 Episode T: 7 Reward: -1.976403\n",
      "Total T: 28299 Episode Num: 2460 Episode T: 7 Reward: -1.188986\n",
      "Total T: 28499 Episode Num: 2480 Episode T: 7 Reward: -1.583249\n",
      "Total T: 28658 Episode Num: 2500 Episode T: 19 Reward: 338.656702\n",
      "Total T: 28833 Episode Num: 2520 Episode T: 7 Reward: -1.731610\n",
      "Total T: 29124 Episode Num: 2540 Episode T: 7 Reward: -1.626901\n",
      "Total T: 29265 Episode Num: 2560 Episode T: 8 Reward: -1.084522\n",
      "Total T: 29412 Episode Num: 2580 Episode T: 7 Reward: -1.796139\n",
      "Total T: 29561 Episode Num: 2600 Episode T: 7 Reward: -1.676112\n",
      "Total T: 29707 Episode Num: 2620 Episode T: 7 Reward: -0.838213\n",
      "Total T: 29910 Episode Num: 2640 Episode T: 7 Reward: -1.411168\n",
      "---------------------------------------\n",
      "Episode_num: 2659: -1.788105\n",
      "---------------------------------------\n",
      "Total T: 30057 Episode Num: 2660 Episode T: 8 Reward: -1.088314\n",
      "Total T: 30475 Episode Num: 2680 Episode T: 7 Reward: -1.297720\n",
      "Total T: 30615 Episode Num: 2700 Episode T: 8 Reward: -41.104688\n",
      "Total T: 30753 Episode Num: 2720 Episode T: 7 Reward: -1.464603\n",
      "Total T: 31114 Episode Num: 2740 Episode T: 7 Reward: -1.575407\n",
      "Total T: 31255 Episode Num: 2760 Episode T: 7 Reward: -1.283023\n",
      "Total T: 31397 Episode Num: 2780 Episode T: 7 Reward: -1.093839\n",
      "Total T: 31582 Episode Num: 2800 Episode T: 6 Reward: -1.564550\n",
      "Total T: 31779 Episode Num: 2820 Episode T: 7 Reward: -1.840619\n",
      "Total T: 32035 Episode Num: 2840 Episode T: 7 Reward: -1.333939\n",
      "Total T: 32188 Episode Num: 2860 Episode T: 7 Reward: -1.515851\n",
      "Total T: 32327 Episode Num: 2880 Episode T: 6 Reward: -1.353108\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s\" % (args.filename, args.policy_name)\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "if not os.path.exists(\"./results\"):\n",
    "    os.makedirs(\"./results\")\n",
    "if not os.path.exists(\"./pytorch_models\"):\n",
    "    os.makedirs(\"./pytorch_models\")\n",
    "\n",
    "## environment set up\n",
    "action_dim = 2\n",
    "\n",
    "\"\"\" Adding the log file \"\"\"\n",
    "logfile = \"%s_%s\" % (args.filename, args.policy_name)\n",
    "log_f = open(\"log_\"+logfile+\".txt\",\"w+\")\n",
    "env_path = '/home/lifan/workspace/RL/needle_master_tools/data/'+ args.filename + '.txt'\n",
    "env = Environment(action_dim,log_f, filename = env_path)\n",
    "\n",
    "state_dim = len(env.gates) + 9\n",
    "\n",
    "\n",
    "\"\"\"\"  for PID controller \"\"\"\n",
    "action_constrain = [10, np.pi/20]\n",
    "parameter = [0.1,0.0009]\n",
    "pid = PID( parameter, env.width, env.height )\n",
    "\n",
    "\"\"\" [lower bound],[higher bound] \"\"\"\n",
    "# env.action_bound = np.array((-1,1)) ## for one dimension action\n",
    "env.action_bound = np.array(([0, -1],[1, 1]))   ## for two dimension action\n",
    "max_action = 1\n",
    "\n",
    "\n",
    "\"\"\" parameters for epsilon declay \"\"\"\n",
    "epsilon_start = 1\n",
    "epsilon_final = 0.01\n",
    "decay_rate = 25000\n",
    "ep_decay = []\n",
    "\n",
    "\"\"\" beta Prioritized Experience Replay\"\"\"\n",
    "beta_start = 0.4\n",
    "beta_frames = 25000\n",
    "\n",
    "\n",
    "### for plotting\n",
    "Reward = []\n",
    "save_path = './out/'\n",
    "\"\"\" start straightly \"\"\"\n",
    "evaluations = []\n",
    "\n",
    "# Initialize policy\n",
    "# policy = TD3.TD3(state_dim, action_dim, max_action)\n",
    "policy = TD3_priorized.TD3(state_dim, action_dim, max_action)\n",
    "# replay_buffer = utils.ReplayBuffer(args.max_size)\n",
    "replay_buffer = utils.NaivePrioritizedBuffer(int(args.max_size))\n",
    "\n",
    "# Evaluate untrained policy\n",
    "# evaluations = [evaluate_policy(policy)]\n",
    "\n",
    "\n",
    "env.total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "done = True\n",
    "\n",
    "while env.total_timesteps < args.max_timesteps:\n",
    "\n",
    "    # Evaluate episode\n",
    "    if timesteps_since_eval >= args.eval_freq:\n",
    "        timesteps_since_eval %= args.eval_freq\n",
    "        evaluations.append(evaluate_policy(policy, log_f))\n",
    "        \n",
    "        if env.last_reward > 100 and env.episode_num > 100: \n",
    "            policy.save(file_name, directory=\"./pytorch_models\")\n",
    "            np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "\n",
    "        continue\n",
    "\n",
    "\n",
    "    ## finish one episode, and train episode_times\n",
    "    if done:\n",
    "#         log_f.write('~~~~~~~~~~~~~~~~~~~~~~~~ iteration {} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n'.format(env.episode_num))\n",
    "\n",
    "\n",
    "        ## load model\n",
    "        # policy.load(file_name,\"./pytorch_models\")\n",
    "\n",
    "        ## training as usual\n",
    "            # if env.total_timesteps != 0 and env.episode_reward > 500:\n",
    "        if env.total_timesteps != 0:\n",
    "            log_f.write('Total:{}, Episode Num:{}, Eposide:{}, Reward:{}\\n'.format(env.total_timesteps, env.episode_num, episode_timesteps, env.episode_reward))\n",
    "            log_f.flush()\n",
    "            \n",
    "            if env.episode_num % 20 == 0:\n",
    "                print ((\"Total T: %d Episode Num: %d Episode T: %d Reward: %f\") % (\n",
    "                env.total_timesteps, env.episode_num, episode_timesteps, env.episode_reward))\n",
    "                env.render( save_image=True, save_path=save_path)\n",
    "\n",
    "        if env.total_timesteps != 0:\n",
    "            beta = min(1.0, beta_start + env.total_timesteps * (1.0 - beta_start) / beta_frames)\n",
    "            policy.train(replay_buffer, episode_timesteps, beta, args.batch_size, \n",
    "                             args.discount, args.tau, args.policy_noise, args.noise_clip, args.policy_freq)\n",
    "\n",
    "        Reward.append(env.episode_reward)\n",
    "\n",
    "        # Reset environment\n",
    "        state = env.reset(log_f)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        env.episode_num += 1\n",
    "        env.episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "\n",
    "    \"\"\" exploration rate decay \"\"\"\n",
    "    args.expl_noise = (epsilon_start - epsilon_final) * math.exp(-1. * env.total_timesteps / decay_rate)\n",
    "    ep_decay.append(args.expl_noise)\n",
    "#     log_f.write('epsilon decay:{}\\n'.format(args.expl_noise))\n",
    "#     if env.total_timesteps % 500 == 0 and args.expl_noise > 0:\n",
    "#         args.expl_noise *= 0.9\n",
    "\n",
    "    \"\"\" alternative between random selected action and policy selected action \"\"\"\n",
    "#     if env.total_timesteps % args.pid_freq < args.pid_interval:\n",
    "# #     if env.total_timesteps < args.pid_interval:\n",
    "#         state_pid = state[0:3]\n",
    "#         action = pid.PIDcontroller( state_pid, env.next_gate, env.gates)\n",
    "# #         log_f.write('PID Action:{}\\n'.format(action))\n",
    "# #         action = env.sample_action()\n",
    "#         # log_f.write('~~~~~~~~~~~random action~~~~~~~~~~\\n')\n",
    "#         # log_f.write('random selected action:{}\\n'.format(action))\n",
    "\n",
    "#     else:\n",
    "#         # print(\"state: \" +str(state))\n",
    "#         action = policy.select_action(state)\n",
    "#         # print(\"select\")\n",
    "#         # log_f.write('~~~~~~~~~~~selected action~~~~~~~~~~\\n')\n",
    "#         log_f.write('Action based on policy:{}\\n'.format(action))\n",
    "#         # print(\"action based on policy:\" + str(action))\n",
    "#         # print(\"action selected: \" +str(action))\n",
    "        \n",
    "#         if args.expl_noise != 0:\n",
    "#             noise = np.random.normal(0, args.expl_noise, size=action_dim)\n",
    "#             # print(\"noise: \" + str(noise))\n",
    "#             action = (action + noise).clip(-1, 1)\n",
    "\n",
    "\n",
    "    \"\"\" using PID controller \"\"\"\n",
    "    # state_pid = state[0:3]\n",
    "    # action = pid.PIDcontroller( state_pid, env.next_gate, env.gates)\n",
    "    # print(\"action based on PID: \" + str(action))\n",
    "\n",
    "    \"\"\" action selected based on pure policy \"\"\"\n",
    "    action = policy.select_action(state)\n",
    "    log_f.write('action based on policy:{}\\n'.format(action))\n",
    "    # print(\"action based on policy:\" + str(action))\n",
    "    if args.expl_noise != 0:\n",
    "#         state_pid = state[0:3]\n",
    "#         guidance = pid.PIDcontroller( state_pid, env.next_gate, env.gates, env.total_timesteps)\n",
    "        noise = np.random.normal(0, args.expl_noise, size=action_dim)\n",
    "        # print(\"noise: \" + str(noise))\n",
    "#         action = ((1 - args.expl_noise) * action + args.expl_noise * guidance)\n",
    "        action = action + noise\n",
    "        action[0] = np.clip(action[0],0,1)\n",
    "        action[1] = np.clip(action[1],-1,1)\n",
    "\n",
    "\n",
    "    ### select action only based on pure RL\n",
    "    # action = policy.select_action(state)\n",
    "    # print(\"action selected: \" +str(action))\n",
    "\n",
    "\n",
    "    # Perform action\n",
    "    new_state, reward, done = env.step(action, log_f)\n",
    "\n",
    "    done_bool = 0 if episode_timesteps + 1 == env.max_time else float(done)\n",
    "    env.episode_reward += reward\n",
    "\n",
    "    # Store data in replay buffer\n",
    "    replay_buffer.add(state, new_state, action, reward, done_bool)\n",
    "    # print(\"state: \" + str(state))\n",
    "    state = new_state\n",
    "\n",
    "    episode_timesteps += 1\n",
    "    env.total_timesteps += 1\n",
    "    timesteps_since_eval += 1\n",
    "\n",
    "plt.plot(range(len(Reward)), np.array(Reward), 'b')\n",
    "plt.savefig('./results/episode reward.png')\n",
    "\n",
    "plt.plot(range(len(policy.actor_loss)), policy.actor_loss, 'b')\n",
    "plt.savefig('./results/actor loss.png')\n",
    "\n",
    "plt.plot(range(len(policy.critic_loss)), policy.critic_loss, 'b')\n",
    "plt.savefig('./results/critic loss.png')\n",
    "\n",
    "plt.plot(range(len(evaluations)), np.array(evaluations), 'b')\n",
    "plt.savefig('./results/evaluation reward.png')\n",
    "print(evaluations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
